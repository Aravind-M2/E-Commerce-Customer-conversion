{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54019f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from scipy.stats import skew\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, chi2_contingency, f_oneway\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "import numpy as np\n",
    "from scipy.stats import boxcox, skew\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c94862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:\\\\GIT HUB\\\\GUVI Mini Proj 4\\\\train_data - train_data.csv\")\n",
    "df_train = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ce6c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26476, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_train, df_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "df_train.shape\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca19ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target column is Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a1851d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The skewness of the order column is: 4.4628 and it is < 1 so highly skewed(positive)\n",
      "The skewness of the price column is: 0.5254 and it is < 0.5 so skewed\n"
     ]
    }
   ],
   "source": [
    "# finding the skewness\n",
    "cont_col = ['order', 'price']\n",
    "for i in cont_col:\n",
    "    skewness_value = skew(df_train[i].values)\n",
    "    if skewness_value > 1:\n",
    "        #(Highly Asymmetrical) | Transformation is strongly recommended.\n",
    "        print(f\"The skewness of the {i} column is: {skewness_value:.4f} and it is < 1 so highly skewed(positive)\")\n",
    "    if skewness_value >= 0.5 and skewness_value <= 1:\n",
    "        #Transformation is often beneficial, but not always critical\n",
    "        print(f\"The skewness of the {i} column is: {skewness_value:.4f} and it is < 0.5 so skewed\")\n",
    "    if skewness_value >= -0.5 and skewness_value <= 0.5:\n",
    "        #n(Approximately Symmetrical) | No transformation typically needed\n",
    "        print(f\"The skewness of the {i} column is: {skewness_value:.4f} and it is near 0  so not skewed\")\n",
    "    if skewness_value <= -1 :\n",
    "        #(Highly Asymmetrical) | Transformation is strongly recommended.\n",
    "        print(f\"The skewness of the {i} column is: {skewness_value:.4f} and it is > -1 so highly skewed(negative)\")\n",
    "    if skewness_value <= -0.5 and skewness_value <= -1:\n",
    "        #Transformation is often beneficial, but not always critical\n",
    "        print(f\"The skewness of the {i} column is: {skewness_value:.4f} and it is > -0.5 so skewed\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cf5e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the previous runs (preprocessing for classification)\n",
    "# Transforming the skewed features using the best transforming methods after analysing the skewness and kurtosis \n",
    "df_train['price_transformed'], optimal_lambda = boxcox(df_train['price'])\n",
    "df_train['order_transformed'] = np.log(df_train['order'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38aa0d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# 2. Function to detect outliers using the IQR method (1.5 * IQR Rule)\n",
    "def detect_iqr_outliers(data, column, factor=1.5):\n",
    "    \"\"\"Detects outliers using the IQR method for a given continuous column.\"\"\"\n",
    "    \n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define the bounds\n",
    "    lower_bound = Q1 - (factor * IQR)\n",
    "    upper_bound = Q3 + (factor * IQR)\n",
    "    \n",
    "    # Identify the outliers\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    \n",
    "    # Report the findings\n",
    "    print(f\"\\n--- Outlier Analysis for '{column}' ---\")\n",
    "    print(f\"Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
    "    print(f\"Lower Bound: {lower_bound:.2f}\")\n",
    "    print(f\"Upper Bound: {upper_bound:.2f}\")\n",
    "    print(f\"Total Outliers Found: {len(outliers)}\")\n",
    "    \n",
    "    if not outliers.empty:\n",
    "        # Show the actual outlier values\n",
    "        print(f\"Outlier Values:\")\n",
    "        print(outliers[[column]])\n",
    "    else:\n",
    "        print(\"No outliers found.\")\n",
    "    \n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# 3. Define the continuous columns\n",
    "# continuous_features = ['Price', 'Area'] \n",
    "\n",
    "# 4. Loop through the continuous columns and apply the detection function\n",
    "# all_outliers = {}\n",
    "# for feature in continuous_features:\n",
    "#     outliers, lower_bound, upper_bound = detect_iqr_outliers(df, feature)\n",
    "#     all_outliers[feature] = {'outliers': outliers, 'lower': lower_bound, 'upper': upper_bound}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6927437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Outlier Analysis for 'order' ---\n",
      "Q1: 2.00, Q3: 12.00, IQR: 10.00\n",
      "Lower Bound: -13.00\n",
      "Upper Bound: 27.00\n",
      "Total Outliers Found: 7308\n",
      "Outlier Values:\n",
      "        order\n",
      "98961      31\n",
      "128684     61\n",
      "71417      35\n",
      "97096      42\n",
      "83715      35\n",
      "...       ...\n",
      "52256      36\n",
      "38660      48\n",
      "106530     72\n",
      "119346     37\n",
      "5311       41\n",
      "\n",
      "[7308 rows x 1 columns]\n",
      "\n",
      "--- Outlier Analysis for 'price' ---\n",
      "Q1: 33.00, Q3: 52.00, IQR: 19.00\n",
      "Lower Bound: 4.50\n",
      "Upper Bound: 80.50\n",
      "Total Outliers Found: 1233\n",
      "Outlier Values:\n",
      "        price\n",
      "60138      82\n",
      "116171     82\n",
      "56288      82\n",
      "68443      82\n",
      "71952      82\n",
      "...       ...\n",
      "18070      82\n",
      "131926     82\n",
      "52733      82\n",
      "13545      82\n",
      "64925      82\n",
      "\n",
      "[1233 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# 4. Loop through the continuous columns and apply the detection function\n",
    "all_outliers = {} # we did not treat the outlier as the order column failed to pass the hypothesis testing\n",
    "for feature in cont_col:\n",
    "    outliers, lower_bound, upper_bound = detect_iqr_outliers(df_train, feature)\n",
    "    all_outliers[feature] = {'outliers': outliers, 'lower': lower_bound, 'upper': upper_bound}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354271ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'page2_clothing_model' - is removed from clothing model as it is going to be target encoded\n",
    "cat_col = ['month', 'day', 'country', 'session_id','page1_main_category', 'colour', 'location', 'model_photography', 'page','price_2']\n",
    "cont_col = ['order_transformed'] #order\n",
    "target_col = ['price_transformed'] # price\n",
    "relevant = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff3c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0676ba2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. CONTINUOUS vs. CONTINUOUS\n",
      "   Pearson's r: -0.072\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** order_transformed is significantly linearly related to Target_Cont.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"1. CONTINUOUS vs. CONTINUOUS\") #discarding the 'order' feature as the pearson's r is weak\n",
    "alpha = 0.05 # Significance Level\n",
    "# cont_col = ['model_target_encoded']\n",
    "for i in cont_col:\n",
    "    correlation, p_value = pearsonr(df_train[i], df_train['price_transformed'])\n",
    "\n",
    "    print(f\"   Pearson's r: {correlation:.3f}\")\n",
    "    print(f\"   P-Value: {p_value:.5f}\")\n",
    "\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ✅ **Conclusion: Reject Null Hypothesis.** {i} is significantly linearly related to Target_Cont.\")\n",
    "        relevant.append(i)\n",
    "    else:\n",
    "        print(f\"   ❌ Conclusion: Fail to Reject Null Hypothesis. {i} is not significantly linearly related to Target_Cont.\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2e822e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. CATEGORICAL vs. CONTINUOUS \n",
      "   F-Statistic: 6.221\n",
      "   P-Value: 0.00005\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** Mean month is significantly different between Default groups.\n",
      "------------------------------\n",
      "   F-Statistic: 1.255\n",
      "   P-Value: 0.15867\n",
      "   ❌ Conclusion: Fail to Reject Null Hypothesis. Mean day is NOT significantly different between Default groups.\n",
      "------------------------------\n",
      "   F-Statistic: 14.326\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** Mean country is significantly different between Default groups.\n",
      "------------------------------\n",
      "   F-Statistic: 1.536\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** Mean session_id is significantly different between Default groups.\n",
      "------------------------------\n",
      "   F-Statistic: 10981.957\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** Mean page1_main_category is significantly different between Default groups.\n",
      "------------------------------\n",
      "   F-Statistic: 1805.168\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** Mean colour is significantly different between Default groups.\n",
      "------------------------------\n",
      "   F-Statistic: 829.663\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** Mean location is significantly different between Default groups.\n",
      "------------------------------\n",
      "   F-Statistic: 6371.658\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** Mean model_photography is significantly different between Default groups.\n",
      "------------------------------\n",
      "   F-Statistic: 1741.033\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** Mean page is significantly different between Default groups.\n",
      "------------------------------\n",
      "   F-Statistic: 163260.505\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** Mean price_2 is significantly different between Default groups.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Regression - target - [price]\n",
    "# ==========================================================\n",
    "# SCENARIO 2: CONTINUOUS vs. CATEGORICAL\n",
    "# Test: ANOVA (Analysis of Variance) - General case for F-Test\n",
    "# Example: Income (Cont. Feature) vs. Target_Cat (Cat. Target)\n",
    "# Goal: Check if mean Income is different across Default groups (0 vs 1)\n",
    "# ==========================================================\n",
    "print(\"2. CATEGORICAL vs. CONTINUOUS \")\n",
    "alpha = 0.05 # Significance Level\n",
    "# Separate the continuous data into groups based on the categorical target\n",
    "for i in cat_col:\n",
    "    group_list = []\n",
    "    for j in data[i].unique():\n",
    "        group = data[data[i] == j]['price']\n",
    "        group_list.append(group)\n",
    "\n",
    "\n",
    "    # The f_oneway test performs ANOVA (for 2+ groups)\n",
    "    f_statistic, p_value = f_oneway(*group_list)\n",
    "\n",
    "    print(f\"   F-Statistic: {f_statistic:.3f}\")\n",
    "    print(f\"   P-Value: {p_value:.5f}\")\n",
    "\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ✅ **Conclusion: Reject Null Hypothesis.** Mean {i} is significantly different between Default groups.\")\n",
    "        relevant.append(i)\n",
    "    else:\n",
    "        print(f\"   ❌ Conclusion: Fail to Reject Null Hypothesis. Mean {i} is NOT significantly different between Default groups.\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ba5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant\n",
    "# ['order_transformed', -- weak - remove\n",
    "#  'month', - cyclic encode\n",
    "#  'country', - group + OHE\n",
    "#  'session_id', -- unique - remove\n",
    "#  'page1_main_category', - OHE\n",
    "#  'colour', - group(not req as only one is below thershold and same number of column will be achieved)+ OHE\n",
    "#  'location', - OHE\n",
    "#  'model_photography', - OHE\n",
    "#  'page', - ordinal (leave as it is)\n",
    "#  'price_2'] - binary\n",
    "# 'page2_clothing_model' - grouping + target encodeing (hypo test and outlier and skew)+ scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping, Target encoding, OHE,Binary,label, cyclic encoding and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28874d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # For demonstration, we'll re-create the DataFrame with high cardinality\n",
    "# # In your real environment, you would load your existing 'df'.\n",
    "# np.random.seed(42)\n",
    "# N = 1000 \n",
    "# countries_list = ['USA'] * 300 + ['Germany'] * 250 + ['UK'] * 150 + ['France'] * 100 \n",
    "# num_rare_countries = 43\n",
    "# rare_country_data = [f'Country_{i}' for i in range(num_rare_countries) for _ in range(5)]\n",
    "# countries_list += rare_country_data \n",
    "# countries_list = countries_list[:N] \n",
    "# np.random.shuffle(countries_list)\n",
    "\n",
    "# models_list = ['Model_A'] * 100 + ['Model_B'] * 80 + ['Model_C'] * 60\n",
    "# num_rare_models = 213\n",
    "# rare_model_data = [f'Model_{i}' for i in range(num_rare_models) for _ in range(4)]\n",
    "# models_list += rare_model_data\n",
    "# models_list = models_list[:N]\n",
    "# np.random.shuffle(models_list)\n",
    "\n",
    "# df = pd.DataFrame({\n",
    "#     'country': countries_list,\n",
    "#     'page2_clothing_model': models_list,\n",
    "#     'target': np.random.randint(0, 2, size=N) \n",
    "# })\n",
    "\n",
    "# --- Main Grouping Function ---\n",
    "def group_rare_categories(df, column_name, min_percentage=0.01, rare_label='RARE_GROUP'):\n",
    "    \"\"\"\n",
    "    Groups categories in a specified column that fall below a minimum\n",
    "    percentage threshold into a single 'RARE_GROUP' and returns the updated DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - column_name (str): The name of the categorical column to process.\n",
    "    - min_percentage (float): The threshold (e.g., 0.01 for 1%).\n",
    "    - rare_label (str): The label to use for the grouped rare categories.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Calculate the normalized frequency counts\n",
    "    value_counts = df[column_name].value_counts(normalize=True)\n",
    "    \n",
    "    # 2. Identify the categories to be grouped\n",
    "    rare_categories = value_counts[value_counts < min_percentage].index.tolist()\n",
    "    \n",
    "    # Check if grouping is necessary\n",
    "    if not rare_categories:\n",
    "        print(f\"No categories in '{column_name}' were below the {min_percentage*100:.2f}% threshold. Skipping.\")\n",
    "        return df\n",
    "    \n",
    "    # 3. Create the new column name\n",
    "    new_column_name = f'{column_name}_grouped'\n",
    "    df[new_column_name] = df[column_name].copy()\n",
    "    \n",
    "    # 4. Apply the grouping using .loc for efficient assignment\n",
    "    df.loc[df[column_name].isin(rare_categories), new_column_name] = rare_label\n",
    "    \n",
    "    # 5. Report summary\n",
    "    original_cardinality = df[column_name].nunique()\n",
    "    new_cardinality = df[new_column_name].nunique()\n",
    "    \n",
    "    print(f\"\\n--- Grouping Summary for '{column_name}' (Threshold: {min_percentage*100:.2f}%) ---\")\n",
    "    print(f\"Original Cardinality: {original_cardinality}\")\n",
    "    print(f\"Categories Grouped into '{rare_label}': {len(rare_categories)}\")\n",
    "    print(f\"New Cardinality: {new_cardinality}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3bef7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Grouping Summary for 'country' (Threshold: 1.00%) ---\n",
      "Original Cardinality: 44\n",
      "Categories Grouped into 'RARE_GROUP': 40\n",
      "New Cardinality: 5\n",
      "\n",
      "--- Grouping Summary for 'page2_clothing_model' (Threshold: 0.50%) ---\n",
      "Original Cardinality: 216\n",
      "Categories Grouped into 'RARE_GROUP': 145\n",
      "New Cardinality: 72\n",
      "\n",
      "--- Final Grouped Data Head ---\n",
      "   country country_grouped page2_clothing_model page2_clothing_model_grouped\n",
      "0       29              29                  C53                   RARE_GROUP\n",
      "1       29              29                   B9                           B9\n",
      "2        9               9                   A9                           A9\n",
      "3       29              29                   A3                           A3\n",
      "4       29              29                  P61                   RARE_GROUP\n",
      "\n",
      "Final Value Counts for 'country_grouped':\n",
      "country_grouped\n",
      "29            85683\n",
      "9             11615\n",
      "RARE_GROUP     4417\n",
      "24             2620\n",
      "46             1568\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final Value Counts for 'page2_clothing_model_grouped':\n",
      "page2_clothing_model_grouped\n",
      "RARE_GROUP    39797\n",
      "B4             2256\n",
      "A2             1907\n",
      "A11            1795\n",
      "P1             1717\n",
      "              ...  \n",
      "C11             545\n",
      "P48             544\n",
      "P33             543\n",
      "B21             536\n",
      "C50             531\n",
      "Name: count, Length: 72, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_2028\\3883849262.py:57: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'RARE_GROUP' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df[column_name].isin(rare_categories), new_column_name] = rare_label\n"
     ]
    }
   ],
   "source": [
    "# A. Grouping 'country' (Original Card: 46)\n",
    "# Using a 1% threshold\n",
    "df = group_rare_categories(df_train, 'country', min_percentage=0.01)\n",
    "\n",
    "# B. Grouping 'page2_clothing_model' (Original Card: 216)\n",
    "# Using a slightly lower 0.5% threshold to ensure more rare models are grouped\n",
    "df = group_rare_categories(df_train, 'page2_clothing_model', min_percentage=0.005)\n",
    "\n",
    "# 3. Final Verification of the New Columns\n",
    "print(\"\\n--- Final Grouped Data Head ---\")\n",
    "print(df[['country', 'country_grouped', 'page2_clothing_model', 'page2_clothing_model_grouped']].head())\n",
    "print(\"\\nFinal Value Counts for 'country_grouped':\")\n",
    "print(df['country_grouped'].value_counts())\n",
    "print(\"\\nFinal Value Counts for 'page2_clothing_model_grouped':\")\n",
    "print(df['page2_clothing_model_grouped'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9924fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8178e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trail and error part starts\n",
    "# the target encoding is needed so even if we delete these codes we will have to re-code a simple one atleast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff2fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target encoding and OHE ---------------- edit this to fit for the continuous feature\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assume 'df' is your DataFrame with 'country_grouped', 'page2_clothing_model_grouped', and 'target'\n",
    "# (The 'target' is your classification outcome, e.g., 'purchased' or 'converted')\n",
    "\n",
    "# --- 1. Target Encoding Setup for 'page2_clothing_model_grouped' ---\n",
    "\n",
    "# We will use StratifiedKFold to ensure the target ratio is preserved in each fold\n",
    "NFOLDS = 5\n",
    "kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a placeholder column for the encoded feature\n",
    "df_train['model_target_encoded'] = np.nan\n",
    "\n",
    "# Perform Target Encoding via K-Fold Cross-Validation\n",
    "for train_index, val_index in kf.split(df_train, df_train['price_transformed']):\n",
    "    # Initialize TargetEncoder (smoothing is default and recommended)\n",
    "    encoder = TargetEncoder()\n",
    "    \n",
    "    # Fit the encoder ONLY on the K-1 folds (train data)\n",
    "    encoder.fit(df_train.iloc[train_index]['page2_clothing_model_grouped'], df_train.iloc[train_index]['price_transformed'])\n",
    "    \n",
    "    # Transform the remaining fold (validation data)\n",
    "    # The transformation uses the means calculated from the train_index\n",
    "    df_train.loc[val_index, 'model_target_encoded'] = encoder.transform(\n",
    "        df_train.iloc[val_index]['page2_clothing_model_grouped']\n",
    "    ).to_numpy().flatten()\n",
    "\n",
    "# For any new data (when predicting), you must fit the encoder on the ENTIRE training set.\n",
    "final_model_encoder = TargetEncoder()\n",
    "final_model_encoder.fit(df_train['page2_clothing_model_grouped'], df_train['price_transformed'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f430f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define the KFold Target Encoder Class ---\n",
    "\n",
    "class KFoldTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Performs K-Fold Target Encoding for a single categorical column \n",
    "    while preventing data leakage using cross-validation.\n",
    "    \n",
    "    This encoder is suitable for regression tasks where the target (y) is continuous.\n",
    "    \n",
    "    Attributes:\n",
    "        _kf (KFold): KFold splitter object.\n",
    "        _mapping (dict): Stores the mean target mapping for use in the 'transform' method.\n",
    "        _global_mean (float): The mean of the entire target variable, used as a fallback\n",
    "                              for new/unseen categories during 'transform'.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=42, smoothing=None):\n",
    "        \"\"\"\n",
    "        Initializes the encoder.\n",
    "        \n",
    "        Args:\n",
    "            n_splits (int): Number of folds for KFold.\n",
    "            shuffle (bool): Whether to shuffle the data before splitting.\n",
    "            random_state (int): Seed for shuffling.\n",
    "            smoothing (float, optional): Optional smoothing parameter (lambda) \n",
    "                                         to blend category mean with global mean.\n",
    "                                         (Not implemented in this basic version but common).\n",
    "        \"\"\"\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        self.smoothing = smoothing\n",
    "        self._kf = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "        self._mapping = {}\n",
    "        self._global_mean = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the encoder. In K-Fold Target Encoding, 'fit' calculates the \n",
    "        overall category means based on the entire training set.\n",
    "\n",
    "        Args:\n",
    "            X (pd.Series or pd.DataFrame): The categorical feature column.\n",
    "            y (pd.Series or np.ndarray): The continuous target column.\n",
    "        \"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "             X = X.iloc[:, 0] # Ensure we only work with the first column if DataFrame is passed\n",
    "             \n",
    "        # Ensure y is a Series for robust use in grouping functions\n",
    "        y_series = pd.Series(y)\n",
    "        \n",
    "        self._global_mean = y_series.mean()\n",
    "        \n",
    "        # FIX: Group the target (y) by the feature (X) values directly.\n",
    "        # This is the most robust, name-agnostic way to calculate the category mean for Series, \n",
    "        # avoiding the KeyError.\n",
    "        self._mapping = y_series.groupby(X).mean().to_dict()\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Applies the fitted mean encoding to new data.\n",
    "        \n",
    "        Args:\n",
    "            X (pd.Series or pd.DataFrame): The categorical feature column to transform.\n",
    "\n",
    "        Returns:\n",
    "            pd.Series: The target encoded feature column.\n",
    "        \"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "             X = X.iloc[:, 0]\n",
    "             \n",
    "        # Use the stored mapping and fill any missing values (new categories) with the global mean\n",
    "        encoded_feature = X.map(self._mapping).fillna(self._global_mean)\n",
    "        return encoded_feature\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Performs the target encoding using K-Fold cross-validation \n",
    "        to generate the encoded training feature without leakage.\n",
    "\n",
    "        Args:\n",
    "            X (pd.Series or pd.DataFrame): The categorical feature column.\n",
    "            y (pd.Series or np.ndarray): The continuous target column.\n",
    "\n",
    "        Returns:\n",
    "            pd.Series: The K-Fold target encoded feature column.\n",
    "        \"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "             X_series = X.iloc[:, 0].copy()\n",
    "        else:\n",
    "             X_series = X.copy()\n",
    "             \n",
    "        y = pd.Series(y)\n",
    "        X_encoded = pd.Series(np.zeros(len(X_series)), index=X_series.index)\n",
    "        \n",
    "        # 1. Iterate over K-Folds\n",
    "        for train_idx, val_idx in self._kf.split(X_series, y):\n",
    "            # Split the current data into training and validation folds\n",
    "            X_train_fold, X_val_fold = X_series.iloc[train_idx], X_series.iloc[val_idx]\n",
    "            y_train_fold = y.iloc[train_idx]\n",
    "            \n",
    "            # 2. Calculate the mean target for each category based ONLY on the current train fold\n",
    "            fold_means = y_train_fold.groupby(X_train_fold).mean()\n",
    "            \n",
    "            # 3. Map these means to the corresponding validation fold\n",
    "            # Use the global mean (calculated from y_train_fold) as fallback for categories \n",
    "            # that only exist in the current validation fold (should be rare/none in ideal CV)\n",
    "            global_mean_fold = y_train_fold.mean()\n",
    "            \n",
    "            X_encoded.iloc[val_idx] = X_val_fold.map(fold_means).fillna(global_mean_fold)\n",
    "            \n",
    "        # 4. Fit the final, overall mapping for future 'transform' calls (on test data)\n",
    "        self.fit(X_series, y) \n",
    "\n",
    "        return X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69340f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = KFoldTargetEncoder(n_splits=5, random_state=42)\n",
    "\n",
    "# 1. Encode the training feature (leakage-free using K-Fold)\n",
    "X_train_encoded = encoder.fit_transform(df_train['page2_clothing_model_grouped'], df_train['price_transformed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3fb7487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         5.108165\n",
       "1         5.529871\n",
       "2         6.632712\n",
       "3         6.355335\n",
       "4         5.108165\n",
       "            ...   \n",
       "105898    5.109845\n",
       "105899    5.080116\n",
       "105900    5.529871\n",
       "105901    4.817257\n",
       "105902    5.315813\n",
       "Length: 105903, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a408b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['model_target_encoded'] = X_train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea7f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Encode the test feature (using the mapping fitted on the *entire* training set) --- check and delete after that\n",
    "X_test_encoded = encoder.transform(X_test['category'])\n",
    "\n",
    "# --- 4. Display Results ---\n",
    "\n",
    "print(\"--- Original Training Data (First 14 rows) ---\")\n",
    "print(pd.concat([X_train, y_train], axis=1).head(7))\n",
    "print(\"\\nGlobal Target Mean (Train Set):\", round(encoder._global_mean, 4))\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(\"\\n--- K-Fold Target Encoded Training Feature ---\")\n",
    "# Combining original category, target, and the new encoded feature for inspection\n",
    "train_result = pd.DataFrame({\n",
    "    'original_category': X_train['category'],\n",
    "    'target_y': y_train,\n",
    "    'encoded_category': X_train_encoded\n",
    "})\n",
    "print(train_result)\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(\"\\n--- Target Encoded Test Feature (Used for Prediction) ---\")\n",
    "test_result = pd.DataFrame({\n",
    "    'original_category': X_test['category'],\n",
    "    'target_y': y_test,\n",
    "    'encoded_category': X_test_encoded\n",
    "})\n",
    "print(test_result)\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(\"\\n--- Final Mapping (Used for Test/New Data) ---\")\n",
    "# These are the values that will be used by the regression model\n",
    "print({k: round(v, 4) for k, v in encoder._mapping.items()})\n",
    "\n",
    "print(\"\\n\\nNote: The 'encoded_category' values in the Training Set differ slightly from the Final Mapping values,\")\n",
    "print(\"because they were calculated as the mean of the *other* K-Folds, preventing leakage.\")\n",
    "print(\"The 'encoded_category' values in the Test Set use the Final Mapping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9685ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trail and error part stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c5978fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding (country_grouped_...) is complete and safe.\n",
      "One-Hot Encoding (page1_main_category_...) is complete and safe.\n",
      "One-Hot Encoding (colour_...) is complete and safe.\n",
      "One-Hot Encoding (location_...) is complete and safe.\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding\n",
    "ohe=['country_grouped','page1_main_category','colour','location']\n",
    "for i in ohe:\n",
    "    # Use pandas get_dummies for simple OHE\n",
    "    df_ohe = pd.get_dummies(df_train[i], prefix=i, dtype=int)\n",
    "\n",
    "    # Concatenate the new OHE columns to the main DataFrame\n",
    "    df_train = pd.concat([df_train.drop(i, axis=1), df_ohe], axis=1)\n",
    "\n",
    "    # Display the result (replace 'page2_clothing_model_grouped' with the encoded column)\n",
    "    # print(\"\\n--- Final Encoded Features Head ---\")\n",
    "    # print(data[['country_RARE_GROUP','model_target_encoded']].head())\n",
    "\n",
    "    \n",
    "    print(f\"One-Hot Encoding ({i}_...) is complete and safe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25ceb1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price_2\n",
       "0    67714\n",
       "1    64665\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Binary - using label encoding\n",
    "df_train['model_photography'] = df_train['model_photography'].map({1:0, 2:1})\n",
    "df_train['model_photography'].value_counts()\n",
    "df_train['price_2'] = df_train['price_2'].map({1:0, 2:1})\n",
    "df_train['price_2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f27029f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cyclical Encoding for Month (P=12 for a full year)\n",
    "P_month = 12\n",
    "df_train['month_sin'] = np.sin(2 * np.pi * df_train['month'] / P_month)\n",
    "df_train['month_cos'] = np.cos(2 * np.pi * df_train['month'] / P_month)\n",
    "# 2. Cyclical Encoding for Day (P=31 for max days in a month)\n",
    "P_day = 31\n",
    "df_train['day_sin'] = np.sin(2 * np.pi * df_train['day'] / P_day)\n",
    "df_train['day_cos'] = np.cos(2 * np.pi * df_train['day'] / P_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a6a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hypothesis testing on the target encoded column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f5ff13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. CONTINUOUS vs. CONTINUOUS\n",
      "   Pearson's r: -0.608\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** model_target_encoded is significantly linearly related to Target_Cont.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"1. CONTINUOUS vs. CONTINUOUS\") #discarding the 'order' feature as the pearson's r is weak\n",
    "alpha = 0.05 # Significance Level\n",
    "cont_col1 = ['model_target_encoded']\n",
    "for i in cont_col1:\n",
    "    correlation, p_value = pearsonr(df_train[i], df_train['price_transformed'])\n",
    "\n",
    "    print(f\"   Pearson's r: {correlation:.3f}\")\n",
    "    print(f\"   P-Value: {p_value:.5f}\")\n",
    "\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ✅ **Conclusion: Reject Null Hypothesis.** {i} is significantly linearly related to Target_Cont.\")\n",
    "        relevant.append(i)\n",
    "    else:\n",
    "        print(f\"   ❌ Conclusion: Fail to Reject Null Hypothesis. {i} is not significantly linearly related to Target_Cont.\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57ddf46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The skewness of the model_target_encoded column is: 0.5547 and it is < 0.5 so skewed\n"
     ]
    }
   ],
   "source": [
    "# Finding the skewness in target encoded column\n",
    "# finding the skewness\n",
    "# cont_col = ['order', 'price']\n",
    "cont_col1 = ['model_target_encoded']\n",
    "for i in cont_col1:\n",
    "    skewness_value = skew(df_train[i].values)\n",
    "    if skewness_value > 1:\n",
    "        #(Highly Asymmetrical) | Transformation is strongly recommended.\n",
    "        print(f\"The skewness of the {i} column is: {skewness_value:.4f} and it is < 1 so highly skewed(positive)\")\n",
    "    if skewness_value >= 0.5 and skewness_value <= 1:\n",
    "        #Transformation is often beneficial, but not always critical\n",
    "        print(f\"The skewness of the {i} column is: {skewness_value:.4f} and it is < 0.5 so skewed\")\n",
    "    if skewness_value >= -0.5 and skewness_value <= 0.5:\n",
    "        #n(Approximately Symmetrical) | No transformation typically needed\n",
    "        print(f\"The skewness of the {i} column is: {skewness_value:.4f} and it is near 0  so not skewed\")\n",
    "    if skewness_value <= -1 :\n",
    "        #(Highly Asymmetrical) | Transformation is strongly recommended.\n",
    "        print(f\"The skewness of the {i} column is: {skewness_value:.4f} and it is > -1 so highly skewed(negative)\")\n",
    "    if skewness_value <= -0.5 and skewness_value <= -1:\n",
    "        #Transformation is often beneficial, but not always critical\n",
    "        print(f\"The skewness of the {i} column is: {skewness_value:.4f} and it is > -0.5 so skewed\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3856e84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Outlier Analysis for 'model_target_encoded' ---\n",
      "Q1: 1.00, Q3: 2.00, IQR: 1.00\n",
      "Lower Bound: -0.50\n",
      "Upper Bound: 3.50\n",
      "Total Outliers Found: 0\n",
      "No outliers found.\n"
     ]
    }
   ],
   "source": [
    "# Outlier detection\n",
    "# 4. Loop through the continuous columns and apply the detection function\n",
    "all_outliers = {}\n",
    "for feature in cont_col1:\n",
    "    outliers, lower_bound, upper_bound = detect_iqr_outliers(df_train, feature)\n",
    "    all_outliers[feature] = {'outliers': outliers, 'lower': lower_bound, 'upper': upper_bound}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77bea37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Scaler Parameters Learned from df_train ---\n",
      "Mean : 1.49\n",
      "Standard Deviation : 0.39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# scaling\n",
    "# scaling on the Target encoded column\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 2. Scaling the Target-Encoded Feature ---\n",
    "\n",
    "# 2a. Instantiate the Scaler\n",
    "# We use StandardScaler here, which transforms the data to have a mean of 0 and a standard deviation of 1.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 2b. FIT the Scaler ONLY on the Training Data\n",
    "# The .fit() method calculates the mean and standard deviation of the 'encoded_feature' \n",
    "# in the training set (X_train).\n",
    "scaler.fit(df_train[['model_target_encoded']])\n",
    "\n",
    "print(\"--- Scaler Parameters Learned from df_train ---\")\n",
    "print(f\"Mean : {scaler.mean_[0]:.2f}\")\n",
    "print(f\"Standard Deviation : {scaler.scale_[0]:.2f}\\n\")\n",
    "\n",
    "# 2c. TRANSFORM the Training Data\n",
    "# Apply the calculated mean and std to transform the training data.\n",
    "df_train['model_target_encoded_scaled'] = scaler.transform(df_train[['model_target_encoded']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f330f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Scaler Parameters Learned from df_train ---\n",
      "Mean : 5.30\n",
      "Standard Deviation : 0.56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this scaling process is for unsupervised learning and \n",
    "# not for Regression - leave it while running for regression\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[['price_transformed']])\n",
    "\n",
    "print(\"--- Scaler Parameters Learned from df_train ---\")\n",
    "print(f\"Mean : {scaler.mean_[0]:.2f}\")\n",
    "print(f\"Standard Deviation : {scaler.scale_[0]:.2f}\\n\")\n",
    "\n",
    "df_train['price_transformed_scaled'] = scaler.transform(df_train[['price_transformed']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874ca22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Scaler Parameters Learned from df_train ---\n",
      "Mean : 1.69\n",
      "Standard Deviation : 1.09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this scaling process is for unsupervised learning and \n",
    "# not for Regression - leave it while running for regression\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[['order_transformed']])\n",
    "\n",
    "print(\"--- Scaler Parameters Learned from df_train ---\")\n",
    "print(f\"Mean : {scaler.mean_[0]:.2f}\")\n",
    "print(f\"Standard Deviation : {scaler.scale_[0]:.2f}\\n\")\n",
    "\n",
    "df_train['order_transformed_scaled'] = scaler.transform(df_train[['order_transformed']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d63155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 132379 entries, 0 to 132378\n",
      "Data columns (total 51 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   year                          132379 non-null  int64  \n",
      " 1   month                         132379 non-null  int64  \n",
      " 2   day                           132379 non-null  int64  \n",
      " 3   order                         132379 non-null  int64  \n",
      " 4   country                       132379 non-null  int64  \n",
      " 5   session_id                    132379 non-null  int64  \n",
      " 6   page2_clothing_model          132379 non-null  object \n",
      " 7   model_photography             132379 non-null  int64  \n",
      " 8   price                         132379 non-null  int64  \n",
      " 9   price_2                       132379 non-null  int64  \n",
      " 10  page                          132379 non-null  int64  \n",
      " 11  price_transformed             132379 non-null  float64\n",
      " 12  order_transformed             132379 non-null  float64\n",
      " 13  page2_clothing_model_grouped  132379 non-null  object \n",
      " 14  model_target_encoded          132379 non-null  float64\n",
      " 15  country_grouped_9             132379 non-null  int64  \n",
      " 16  country_grouped_24            132379 non-null  int64  \n",
      " 17  country_grouped_29            132379 non-null  int64  \n",
      " 18  country_grouped_46            132379 non-null  int64  \n",
      " 19  country_grouped_RARE_GROUP    132379 non-null  int64  \n",
      " 20  page1_main_category_1         132379 non-null  int64  \n",
      " 21  page1_main_category_2         132379 non-null  int64  \n",
      " 22  page1_main_category_3         132379 non-null  int64  \n",
      " 23  page1_main_category_4         132379 non-null  int64  \n",
      " 24  colour_1                      132379 non-null  int64  \n",
      " 25  colour_2                      132379 non-null  int64  \n",
      " 26  colour_3                      132379 non-null  int64  \n",
      " 27  colour_4                      132379 non-null  int64  \n",
      " 28  colour_5                      132379 non-null  int64  \n",
      " 29  colour_6                      132379 non-null  int64  \n",
      " 30  colour_7                      132379 non-null  int64  \n",
      " 31  colour_8                      132379 non-null  int64  \n",
      " 32  colour_9                      132379 non-null  int64  \n",
      " 33  colour_10                     132379 non-null  int64  \n",
      " 34  colour_11                     132379 non-null  int64  \n",
      " 35  colour_12                     132379 non-null  int64  \n",
      " 36  colour_13                     132379 non-null  int64  \n",
      " 37  colour_14                     132379 non-null  int64  \n",
      " 38  location_1                    132379 non-null  int64  \n",
      " 39  location_2                    132379 non-null  int64  \n",
      " 40  location_3                    132379 non-null  int64  \n",
      " 41  location_4                    132379 non-null  int64  \n",
      " 42  location_5                    132379 non-null  int64  \n",
      " 43  location_6                    132379 non-null  int64  \n",
      " 44  month_sin                     132379 non-null  float64\n",
      " 45  month_cos                     132379 non-null  float64\n",
      " 46  day_sin                       132379 non-null  float64\n",
      " 47  day_cos                       132379 non-null  float64\n",
      " 48  model_target_encoded_scaled   132379 non-null  float64\n",
      " 49  price_transformed_scaled      132379 non-null  float64\n",
      " 50  order_transformed_scaled      132379 non-null  float64\n",
      "dtypes: float64(10), int64(39), object(2)\n",
      "memory usage: 51.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()\n",
    "# use binary encoding and explore the PCA more - future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7790870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this scaling process is for unsupervised learning and \n",
    "# not for Regression - leave it while running for regression\n",
    "df_train.to_csv('df_train_preprocessed_unsupervised_learning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c5b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06c14d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 26476 entries, 31956 to 34878\n",
      "Data columns (total 14 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   year                  26476 non-null  int64 \n",
      " 1   month                 26476 non-null  int64 \n",
      " 2   day                   26476 non-null  int64 \n",
      " 3   order                 26476 non-null  int64 \n",
      " 4   country               26476 non-null  int64 \n",
      " 5   session_id            26476 non-null  int64 \n",
      " 6   page1_main_category   26476 non-null  int64 \n",
      " 7   page2_clothing_model  26476 non-null  object\n",
      " 8   colour                26476 non-null  int64 \n",
      " 9   location              26476 non-null  int64 \n",
      " 10  model_photography     26476 non-null  int64 \n",
      " 11  price                 26476 non-null  int64 \n",
      " 12  price_2               26476 non-null  int64 \n",
      " 13  page                  26476 non-null  int64 \n",
      "dtypes: int64(13), object(1)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14a3a9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_12128\\2687283577.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_temp.drop_duplicates(inplace= True)\n"
     ]
    }
   ],
   "source": [
    "# target encoding of model in data_test\n",
    "page2_groups = ['RARE_GROUP', 'B9', 'A9', 'A3', 'P1', 'C56', 'A31', 'C17', 'C49',\n",
    "       'B31', 'A12', 'C50', 'C12', 'B16', 'A8', 'A5', 'A14', 'C40', 'A17',\n",
    "       'P16', 'A21', 'A11', 'A7', 'A15', 'P17', 'P2', 'A4', 'B1', 'P33',\n",
    "       'P15', 'B23', 'C1', 'B12', 'B13', 'B4', 'P48', 'A16', 'B30', 'P4',\n",
    "       'P12', 'A18', 'P23', 'B10', 'A10', 'A2', 'B17', 'C14', 'B11',\n",
    "       'A13', 'A1', 'A6', 'C5', 'A33', 'C9', 'B15', 'C7', 'C2', 'B26',\n",
    "       'B3', 'B32', 'C11', 'P3', 'C8', 'B27', 'P6', 'B2', 'C13', 'B24',\n",
    "       'B14', 'B19', 'C4', 'B21']\n",
    "df_test['page2_clothing_model_grouped'] = df_test['page2_clothing_model'].apply(lambda x: 'RARE_GROUP' if x not in page2_groups else x)\n",
    "# df_test['page2_clothing_model_grouped'].unique()\n",
    "\n",
    "df_temp = df_train[['page2_clothing_model_grouped','model_target_encoded']]\n",
    "\n",
    "# df_temp.duplicated().sum()\n",
    "df_temp.drop_duplicates(inplace= True)\n",
    "# df_temp.shape\n",
    "# df_test.shape\n",
    "\n",
    "# 2. Create the dictionary (key is the merge column, value is the column to transfer)\n",
    "# Only include the columns you need for the mapping\n",
    "map_dict = df_temp.set_index('page2_clothing_model_grouped')['model_target_encoded'].to_dict()\n",
    "# map_dict\n",
    "\n",
    "# 3. Use the .map() method to apply the new column to df_test\n",
    "# This is much cleaner and faster than using a lambda/apply for this operation.\n",
    "df_test['model_target_encoded'] = df_test['page2_clothing_model_grouped'].map(map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "547c27e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2d. TRANSFORM the Test Data using the SAME Scaler\n",
    "# This is the critical step to prevent data leakage. We use the mean/std \n",
    "# calculated from the training set to scale the test data.\n",
    "df_test['model_target_encoded_scaled'] = scaler.transform(df_test[['model_target_encoded']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca6a7046",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_grouped = [9,24,29,46]\n",
    "df_test['country_grouped'] = df_test['country'].apply(lambda x: 'RARE_GROUP' if x not in countries_grouped else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8acecee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding (country_grouped_...) is complete and safe.\n",
      "One-Hot Encoding (page1_main_category_...) is complete and safe.\n",
      "One-Hot Encoding (colour_...) is complete and safe.\n",
      "One-Hot Encoding (location_...) is complete and safe.\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding\n",
    "ohe=['country_grouped','page1_main_category','colour','location']\n",
    "for i in ohe:\n",
    "    # Use pandas get_dummies for simple OHE\n",
    "    df_ohe = pd.get_dummies(df_test[i], prefix=i, dtype=int)\n",
    "\n",
    "    # Concatenate the new OHE columns to the main DataFrame\n",
    "    df_test = pd.concat([df_test.drop(i, axis=1), df_ohe], axis=1)\n",
    "\n",
    "    # Display the result (replace 'page2_clothing_model_grouped' with the encoded column)\n",
    "    # print(\"\\n--- Final Encoded Features Head ---\")\n",
    "    # print(data[['country_RARE_GROUP','model_target_encoded']].head())\n",
    "\n",
    "    \n",
    "    print(f\"One-Hot Encoding ({i}_...) is complete and safe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d0f71e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cyclical Encoding for Month (P=12 for a full year)\n",
    "P_month = 12\n",
    "df_test['month_sin'] = np.sin(2 * np.pi * df_test['month'] / P_month)\n",
    "df_test['month_cos'] = np.cos(2 * np.pi * df_test['month'] / P_month)\n",
    "# 2. Cyclical Encoding for Day (P=31 for max days in a month)\n",
    "P_day = 31\n",
    "df_test['day_sin'] = np.sin(2 * np.pi * df_test['day'] / P_day)\n",
    "df_test['day_cos'] = np.cos(2 * np.pi * df_test['day'] / P_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1247c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price_2\n",
       "0    13556\n",
       "1    12920\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Binary - using label encoding\n",
    "df_test['model_photography'] = df_test['model_photography'].map({1:0, 2:1})\n",
    "df_test['model_photography'].value_counts()\n",
    "df_test['price_2'] = df_test['price_2'].map({1:0, 2:1})\n",
    "df_test['price_2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "131d8fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation\n",
    "# Apply the SAME optimal_lambda used in train data to the test data\n",
    "# optimal_lambda is taken from the transformation performed on the train dataset\n",
    "df_test['price_transformed'] = boxcox(df_test['price'], lmbda=optimal_lambda) #------------ this is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9a89ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = ['year','month','day','session_id','country','order','price','page2_clothing_model_grouped','page2_clothing_model',\n",
    "          'model_target_encoded','order_transformed']\n",
    "\n",
    "df_train.drop(columns=remove, inplace=True)\n",
    "df_test.drop(columns=remove, inplace=True) # remove the 'order_transformed' from the remove list while running this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "443cda43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[df_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "36a5cff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns == df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "90eacebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('df_test_preprocessed_regression.csv')\n",
    "df_train.to_csv('df_train_preprocessed_regression.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clickstream_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
