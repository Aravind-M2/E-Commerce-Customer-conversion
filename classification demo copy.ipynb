{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ffb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from scipy.stats import skew\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, chi2_contingency, f_oneway\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "import numpy as np\n",
    "from scipy.stats import boxcox, skew\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06364a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:\\\\GIT HUB\\\\GUVI Mini Proj 4\\\\train_data - train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc0342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train, df_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "# df_train.shape\n",
    "# df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5fa2af79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we updated the frequency encoding with target encoding\n",
    "# model_en = data['page2_clothing_model'].value_counts()\n",
    "# data['model_encoded'] = data['page2_clothing_model'].map(model_en)\n",
    "# data.drop(columns='page2_clothing_model',inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "44ba9af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The skewness of the order column is: 4.4628 and it is < 1 so highly skewed(positive)\n",
      "The skewness of the price column is: 0.5254 and it is < 0.5 so skewed\n"
     ]
    }
   ],
   "source": [
    "# finding the skewness\n",
    "cont_col = ['order', 'price']\n",
    "for i in cont_col:\n",
    "    skewness_value = skew(df_train[i].values)\n",
    "    if skewness_value > 1:\n",
    "        #(Highly Asymmetrical) | Transformation is strongly recommended.\n",
    "        print(f\"The skewness of the {i} column is: {skewness_value:.4f} and it is < 1 so highly skewed(positive)\")\n",
    "    if skewness_value >= 0.5 and skewness_value <= 1:\n",
    "        #Transformation is often beneficial, but not always critical\n",
    "        print(f\"The skewness of the {i} column is: {skewness_value:.4f} and it is < 0.5 so skewed\")\n",
    "    if skewness_value >= -0.5 and skewness_value <= 0.5:\n",
    "        #n(Approximately Symmetrical) | No transformation typically needed\n",
    "        print(f\"The skewness of the {i} column is: {skewness_value:.4f} and it is near 0  so not skewed\")\n",
    "    if skewness_value <= -1 :\n",
    "        #(Highly Asymmetrical) | Transformation is strongly recommended.\n",
    "        print(f\"The skewness of the {i} column is: {skewness_value:.4f} and it is > -1 so highly skewed(negative)\")\n",
    "    if skewness_value <= -0.5 and skewness_value <= -1:\n",
    "        #Transformation is often beneficial, but not always critical\n",
    "        print(f\"The skewness of the {i} column is: {skewness_value:.4f} and it is > -0.5 so skewed\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "441d1d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_2720\\4244884290.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  transform_df[f'{i}_log'] = np.log(transform_df[i])\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_2720\\4244884290.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  transform_df[f'{i}_sqrt'] = np.sqrt(transform_df[i])\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_2720\\4244884290.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  transform_df[f'{i}_cbrt'] = np.cbrt(transform_df[i])\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_2720\\4244884290.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  transform_df[f'{i}_boxcox'], optimal_lambda = boxcox(transform_df[i])\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_2720\\4244884290.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  transform_df[f'{i}_log'] = np.log(transform_df[i])\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_2720\\4244884290.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  transform_df[f'{i}_sqrt'] = np.sqrt(transform_df[i])\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_2720\\4244884290.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  transform_df[f'{i}_cbrt'] = np.cbrt(transform_df[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Skewness Comparison ---\n",
      "Original order Skew: 4.4628\n",
      "Log Transformed Skew: 0.1333\n",
      "Square Root Skew: 1.6300\n",
      "Cube Root Skew: 1.0294\n",
      "Box-Cox Transformed Skew: 0.0136\n",
      "\n",
      "Optimal Lambda (Box-Cox): -0.0548\n",
      "--- Skewness Comparison ---\n",
      "Original price Skew: 0.5254\n",
      "Log Transformed Skew: -0.1245\n",
      "Square Root Skew: 0.2049\n",
      "Cube Root Skew: 0.0967\n",
      "Box-Cox Transformed Skew: -0.0078\n",
      "\n",
      "Optimal Lambda (Box-Cox): 0.1745\n",
      "\n",
      "--- Transformed Data Head ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_2720\\4244884290.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  transform_df[f'{i}_boxcox'], optimal_lambda = boxcox(transform_df[i])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order</th>\n",
       "      <th>price</th>\n",
       "      <th>order_log</th>\n",
       "      <th>order_sqrt</th>\n",
       "      <th>order_cbrt</th>\n",
       "      <th>order_boxcox</th>\n",
       "      <th>price_log</th>\n",
       "      <th>price_sqrt</th>\n",
       "      <th>price_cbrt</th>\n",
       "      <th>price_boxcox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78260</th>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.259921</td>\n",
       "      <td>0.680154</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>6.164414</td>\n",
       "      <td>3.361975</td>\n",
       "      <td>5.080116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5753</th>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>1.817121</td>\n",
       "      <td>1.706646</td>\n",
       "      <td>3.871201</td>\n",
       "      <td>6.928203</td>\n",
       "      <td>3.634241</td>\n",
       "      <td>5.529871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60138</th>\n",
       "      <td>5</td>\n",
       "      <td>82</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>2.236068</td>\n",
       "      <td>1.709976</td>\n",
       "      <td>1.540539</td>\n",
       "      <td>4.406719</td>\n",
       "      <td>9.055385</td>\n",
       "      <td>4.344481</td>\n",
       "      <td>6.632712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115851</th>\n",
       "      <td>3</td>\n",
       "      <td>72</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>1.442250</td>\n",
       "      <td>1.066212</td>\n",
       "      <td>4.276666</td>\n",
       "      <td>8.485281</td>\n",
       "      <td>4.160168</td>\n",
       "      <td>6.355335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84009</th>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>1.912931</td>\n",
       "      <td>1.845799</td>\n",
       "      <td>3.332205</td>\n",
       "      <td>5.291503</td>\n",
       "      <td>3.036589</td>\n",
       "      <td>4.519170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        order  price  order_log  order_sqrt  order_cbrt  order_boxcox  \\\n",
       "78260       2     38   0.693147    1.414214    1.259921      0.680154   \n",
       "5753        6     48   1.791759    2.449490    1.817121      1.706646   \n",
       "60138       5     82   1.609438    2.236068    1.709976      1.540539   \n",
       "115851      3     72   1.098612    1.732051    1.442250      1.066212   \n",
       "84009       7     28   1.945910    2.645751    1.912931      1.845799   \n",
       "\n",
       "        price_log  price_sqrt  price_cbrt  price_boxcox  \n",
       "78260    3.637586    6.164414    3.361975      5.080116  \n",
       "5753     3.871201    6.928203    3.634241      5.529871  \n",
       "60138    4.406719    9.055385    4.344481      6.632712  \n",
       "115851   4.276666    8.485281    4.160168      6.355335  \n",
       "84009    3.332205    5.291503    3.036589      4.519170  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perfroming transformation on skewed features\n",
    "import numpy as np\n",
    "from scipy.stats import boxcox, skew\n",
    "cont_col = ['order', 'price']\n",
    "# --- 2. Perform Transformations ---\n",
    "transform_df = df_train[['order','price']] \n",
    "\n",
    "for i in cont_col:\n",
    "    # 1. Log Transformation (np.log) - Most aggressive\n",
    "    transform_df[f'{i}_log'] = np.log(transform_df[i])\n",
    "\n",
    "    # 2. Square Root Transformation (np.sqrt) - Milder\n",
    "    transform_df[f'{i}_sqrt'] = np.sqrt(transform_df[i])\n",
    "\n",
    "    # 3. Cube Root Transformation (np.cbrt or power 1/3) - Milder than Log\n",
    "    transform_df[f'{i}_cbrt'] = np.cbrt(transform_df[i])\n",
    "\n",
    "    # 4. Box-Cox Transformation - Finds the optimal power (lambda)\n",
    "    # Note: boxcox() returns two things: the transformed data and the optimal lambda.\n",
    "    transform_df[f'{i}_boxcox'], optimal_lambda = boxcox(transform_df[i])\n",
    "\n",
    "\n",
    "# --- 3. Display Results (New Columns and Skewness) ---\n",
    "\n",
    "    print(\"--- Skewness Comparison ---\")\n",
    "    print(f\"Original {i} Skew: {skew(transform_df[i].values):.4f}\")\n",
    "    print(f\"Log Transformed Skew: {skew(transform_df[f'{i}_log'].values):.4f}\")\n",
    "    print(f\"Square Root Skew: {skew(transform_df[f'{i}_sqrt'].values):.4f}\")\n",
    "    print(f\"Cube Root Skew: {skew(transform_df[f'{i}_cbrt'].values):.4f}\")\n",
    "    print(f\"Box-Cox Transformed Skew: {skew(transform_df[f'{i}_boxcox'].values):.4f}\")\n",
    "    print(f\"\\nOptimal Lambda (Box-Cox): {optimal_lambda:.4f}\")\n",
    "\n",
    "print(\"\\n--- Transformed Data Head ---\")\n",
    "transform_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "aff0102c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excess Kurtosis (order): 32.5589\n",
      "Excess Kurtosis (order_log): -0.6032\n",
      "Excess Kurtosis (order_sqrt): 4.2823\n",
      "Excess Kurtosis (order_cbrt): 1.4693\n",
      "Excess Kurtosis (order_boxcox): -0.7015\n",
      "Excess Kurtosis (price): -0.1400\n",
      "Excess Kurtosis (price_log): -0.4241\n",
      "Excess Kurtosis (price_sqrt): -0.4481\n",
      "Excess Kurtosis (price_cbrt): -0.4776\n",
      "Excess Kurtosis (price_boxcox): -0.4712\n"
     ]
    }
   ],
   "source": [
    "# Finding the kurtosis on the features\n",
    "from scipy.stats import kurtosis\n",
    "cont_col1 = ['order','order_log', 'order_sqrt', 'order_cbrt','order_boxcox', 'price', 'price_log', 'price_sqrt', 'price_cbrt','price_boxcox']\n",
    "for i in cont_col1:\n",
    "    original_kurtosis = kurtosis(transform_df[i].values)\n",
    "\n",
    "    print(f\"Excess Kurtosis ({i}): {original_kurtosis:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e5667f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the skewed features using the best transforming methods after analysing the skewness and kurtosis \n",
    "df_train['price_transformed'], optimal_lambda = boxcox(df_train['price'])\n",
    "df_train['order_transformed'] = np.log(df_train['order'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc7e243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = ['month', 'day', 'country', 'session_id','page2_clothing_model',\n",
    "           'page1_main_category', 'colour', 'location', 'model_photography', 'page']\n",
    "cont_col = ['order','price','price_transformed','order_transformed']\n",
    "target_col = ['price_2']\n",
    "relevant = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5a67b271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. CONTINUOUS vs. CATEGORICAL (Income vs. Target_Cat)\n",
      "   F-Statistic: 0.013\n",
      "   P-Value: 0.90808\n",
      "   ❌ Conclusion: Fail to Reject Null Hypothesis. Mean order is NOT significantly different between Default groups.\n",
      "------------------------------\n",
      "   F-Statistic: 130218.283\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** Mean price is significantly different between Price_2 groups.\n",
      "------------------------------\n",
      "   F-Statistic: 138635.954\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** Mean price_transformed is significantly different between Price_2 groups.\n",
      "------------------------------\n",
      "   F-Statistic: 2.561\n",
      "   P-Value: 0.10956\n",
      "   ❌ Conclusion: Fail to Reject Null Hypothesis. Mean order_transformed is NOT significantly different between Default groups.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#hypothesis testing\n",
    "# ==========================================================\n",
    "# SCENARIO 2: CONTINUOUS vs. CATEGORICAL\n",
    "# Test: ANOVA (Analysis of Variance) - General case for F-Test\n",
    "# Example: Income (Cont. Feature) vs. Target_Cat (Cat. Target)\n",
    "# Goal: Check if mean Income is different across Default groups (0 vs 1)\n",
    "# ==========================================================\n",
    "alpha = 0.05 # Significance Level\n",
    "\n",
    "print(\"2. CONTINUOUS vs. CATEGORICAL (Income vs. Target_Cat)\")\n",
    "\n",
    "# Separate the continuous data into groups based on the categorical target\n",
    "# group0 = df[df['Target_Cat'] == 0]['Income']\n",
    "# group1 = df[df['Target_Cat'] == 1]['Income']\n",
    "\n",
    "for i in cont_col:\n",
    "    group1 = df_train[df_train['price_2'] == 1][i]\n",
    "    group2 = df_train[df_train['price_2'] == 2][i]\n",
    "    # The f_oneway test performs ANOVA (for 2+ groups)\n",
    "    f_statistic, p_value = f_oneway(group1, group2)\n",
    "    print(f\"   F-Statistic: {f_statistic:.3f}\")\n",
    "    print(f\"   P-Value: {p_value:.5f}\")\n",
    "\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ✅ **Conclusion: Reject Null Hypothesis.** Mean {i} is significantly different between Price_2 groups.\")\n",
    "        relevant.append(i)\n",
    "    else:\n",
    "        print(f\"   ❌ Conclusion: Fail to Reject Null Hypothesis. Mean {i} is NOT significantly different between Default groups.\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8c09cd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. CATEGORICAL vs. CATEGORICAL (Region vs. Target_Cat)\n",
      "   Contingency Table:\n",
      "price_2      1      2\n",
      "month                \n",
      "4        15878  15069\n",
      "5        11499  11488\n",
      "6        10307  10312\n",
      "7        11562  10751\n",
      "8         4912   4125\n",
      "   Chi-Square Stat: 64.222\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** month and Price_2 are dependent (relevant).\n",
      "------------------------------\n",
      "   Contingency Table:\n",
      "price_2     1     2\n",
      "day                \n",
      "1        2382  2153\n",
      "2        2427  2235\n",
      "3        1974  1906\n",
      "4        2139  1979\n",
      "5        2078  1884\n",
      "6        2047  1961\n",
      "7        2038  1888\n",
      "8        2164  2083\n",
      "9        1969  1863\n",
      "10       1996  1982\n",
      "11       2070  2009\n",
      "12       2061  1968\n",
      "13       1376  1393\n",
      "14       1786  1670\n",
      "15       1493  1490\n",
      "16       1838  1807\n",
      "17       1634  1606\n",
      "18       1574  1482\n",
      "19       1519  1508\n",
      "20       1491  1413\n",
      "21       1630  1541\n",
      "22       1660  1444\n",
      "23       1662  1620\n",
      "24       1600  1675\n",
      "25       1520  1532\n",
      "26       1357  1282\n",
      "27       1383  1299\n",
      "28       1525  1439\n",
      "29       1748  1763\n",
      "30       1440  1323\n",
      "31        577   547\n",
      "   Chi-Square Stat: 38.899\n",
      "   P-Value: 0.12800\n",
      "   ❌ Conclusion: Fail to Reject Null Hypothesis. day and Price_2 are independent (not relevant).\n",
      "------------------------------\n",
      "   Contingency Table:\n",
      "price_2      1      2\n",
      "country              \n",
      "1            2      1\n",
      "2           19     23\n",
      "3           79     72\n",
      "5            1      3\n",
      "6            0      2\n",
      "7            3      4\n",
      "9         5631   5984\n",
      "10          50     50\n",
      "11          37     34\n",
      "12          64     75\n",
      "13           0      1\n",
      "14          19     11\n",
      "15          33     45\n",
      "16         276    255\n",
      "17           8      5\n",
      "18           4      8\n",
      "19           6     12\n",
      "20           3      4\n",
      "21         271    241\n",
      "22          52     44\n",
      "23           9     17\n",
      "24        1338   1282\n",
      "25          27     15\n",
      "26           1      0\n",
      "27          91     70\n",
      "28          46     45\n",
      "29       44090  41593\n",
      "30           1      2\n",
      "31          58     42\n",
      "32          18     11\n",
      "33           0      1\n",
      "34         260    211\n",
      "35           1      0\n",
      "36           7      4\n",
      "37          42     61\n",
      "38          52     40\n",
      "39          11      5\n",
      "41         221    205\n",
      "42          55     53\n",
      "43           8     12\n",
      "44         454    438\n",
      "45           3      2\n",
      "46         807    761\n",
      "47           0      1\n",
      "   Chi-Square Stat: 81.071\n",
      "   P-Value: 0.00040\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** country and Price_2 are dependent (relevant).\n",
      "------------------------------\n",
      "   Contingency Table:\n",
      "price_2     1  2\n",
      "session_id      \n",
      "1           5  2\n",
      "2           6  2\n",
      "3           2  1\n",
      "4           2  2\n",
      "5           1  0\n",
      "...        .. ..\n",
      "24021       1  2\n",
      "24022       0  2\n",
      "24023       0  3\n",
      "24024       1  0\n",
      "24026       2  1\n",
      "\n",
      "[21613 rows x 2 columns]\n",
      "   Chi-Square Stat: 23830.593\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** session_id and Price_2 are dependent (relevant).\n",
      "------------------------------\n",
      "   Contingency Table:\n",
      "price_2                  1     2\n",
      "page2_clothing_model            \n",
      "A1                       0  1472\n",
      "A10                      0  1494\n",
      "A11                   1795     0\n",
      "A12                      0  1262\n",
      "A13                      0  1039\n",
      "...                    ...   ...\n",
      "P8                       0   420\n",
      "P80                      0   130\n",
      "P81                    138     0\n",
      "P82                    470     0\n",
      "P9                       0   396\n",
      "\n",
      "[216 rows x 2 columns]\n",
      "   Chi-Square Stat: 105903.000\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** page2_clothing_model and Price_2 are dependent (relevant).\n",
      "------------------------------\n",
      "   Contingency Table:\n",
      "price_2                  1      2\n",
      "page1_main_category              \n",
      "1                    10894  20990\n",
      "2                    15683   8876\n",
      "3                    14083  10556\n",
      "4                    13498  11323\n",
      "   Chi-Square Stat: 5727.030\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** page1_main_category and Price_2 are dependent (relevant).\n",
      "------------------------------\n",
      "   Contingency Table:\n",
      "price_2      1     2\n",
      "colour              \n",
      "1          891  4161\n",
      "2        11784  7242\n",
      "3         8835  9875\n",
      "4         5817  4782\n",
      "5          295   759\n",
      "6         4079  7174\n",
      "7          974  3348\n",
      "8            0  1727\n",
      "9         4738  3959\n",
      "10        2256     0\n",
      "11        2892   411\n",
      "12        4530  1098\n",
      "13        2093  1966\n",
      "14        4974  5243\n",
      "   Chi-Square Stat: 13691.631\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** colour and Price_2 are dependent (relevant).\n",
      "------------------------------\n",
      "   Contingency Table:\n",
      "price_2       1      2\n",
      "location              \n",
      "1         12704   9343\n",
      "2         10900  10360\n",
      "3          7977   5892\n",
      "4          7766   9723\n",
      "5          8137   9740\n",
      "6          6674   6687\n",
      "   Chi-Square Stat: 1147.892\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** location and Price_2 are dependent (relevant).\n",
      "------------------------------\n",
      "   Contingency Table:\n",
      "price_2                1      2\n",
      "model_photography              \n",
      "1                  41822  36579\n",
      "2                  12336  15166\n",
      "   Chi-Square Stat: 586.818\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** model_photography and Price_2 are dependent (relevant).\n",
      "------------------------------\n",
      "   Contingency Table:\n",
      "price_2      1      2\n",
      "page                 \n",
      "1        31368  28428\n",
      "2        14413  11900\n",
      "3         3921   8445\n",
      "4         3201   2432\n",
      "5         1255    540\n",
      "   Chi-Square Stat: 2375.661\n",
      "   P-Value: 0.00000\n",
      "   ✅ **Conclusion: Reject Null Hypothesis.** page and Price_2 are dependent (relevant).\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#hypothesis testing\n",
    "# ==========================================================\n",
    "# SCENARIO 3: CATEGORICAL vs. CATEGORICAL\n",
    "# Test: Chi-Square Test\n",
    "# Example: Region (Cat. Feature) vs. Target_Cat (Cat. Target)\n",
    "# Goal: Check if the distribution of Target_Cat is dependent on Region\n",
    "# ==========================================================\n",
    "\n",
    "print(\"3. CATEGORICAL vs. CATEGORICAL (Region vs. Target_Cat)\")\n",
    "\n",
    "alpha = 0.05 # Significance Level\n",
    "# Create a Contingency Table (crosstab)\n",
    "# contingency_table = pd.crosstab(df['Region'], df['Target_Cat'])\n",
    "\n",
    "for i in cat_col:\n",
    "    contingency_table = pd.crosstab(df_train[i], df_train['price_2'])\n",
    "\n",
    "    # The chi2_contingency test returns: chi2 stat, p-value, df, expected freqs\n",
    "    chi2_stat, p_value, degrees_of_freedom, expected_freqs = chi2_contingency(contingency_table)\n",
    "\n",
    "    print(\"   Contingency Table:\")\n",
    "    print(contingency_table)\n",
    "    print(f\"   Chi-Square Stat: {chi2_stat:.3f}\")\n",
    "    print(f\"   P-Value: {p_value:.5f}\")\n",
    "\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ✅ **Conclusion: Reject Null Hypothesis.** {i} and Price_2 are dependent (relevant).\")\n",
    "        relevant.append(i)\n",
    "    else:\n",
    "        print(f\"   ❌ Conclusion: Fail to Reject Null Hypothesis. {i} and Price_2 are independent (not relevant).\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5192c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant features\n",
    "# ['price', --- not req\n",
    "#  'price_transformed', --- cont\n",
    "#  'month', --- OHE\n",
    "#  'country', --- Target encode\n",
    "#  'session_id', --- unique (not req)\n",
    "#  'page2_clothing_model', --- Target encode - already done a frequecny\n",
    "#  'page1_main_category', --- OHE\n",
    "#  'colour', --- OHE\n",
    "#  'location', --- OHE\n",
    "#  'model_photography', --- OHE\n",
    "#  'page', --- no encoding(ordinal)\n",
    "#  'model_encoded'] --- already done a frequecny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0ac06cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>order</th>\n",
       "      <th>country</th>\n",
       "      <th>session_id</th>\n",
       "      <th>page1_main_category</th>\n",
       "      <th>page2_clothing_model</th>\n",
       "      <th>colour</th>\n",
       "      <th>location</th>\n",
       "      <th>model_photography</th>\n",
       "      <th>price</th>\n",
       "      <th>price_2</th>\n",
       "      <th>page</th>\n",
       "      <th>price_transformed</th>\n",
       "      <th>order_transformed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78260</th>\n",
       "      <td>2008</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>17592</td>\n",
       "      <td>3</td>\n",
       "      <td>C53</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5.080116</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5753</th>\n",
       "      <td>2008</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>12203</td>\n",
       "      <td>2</td>\n",
       "      <td>B9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5.529871</td>\n",
       "      <td>1.791759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60138</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3938</td>\n",
       "      <td>1</td>\n",
       "      <td>A9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.632712</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115851</th>\n",
       "      <td>2008</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>17794</td>\n",
       "      <td>1</td>\n",
       "      <td>A3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.355335</td>\n",
       "      <td>1.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84009</th>\n",
       "      <td>2008</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>15077</td>\n",
       "      <td>4</td>\n",
       "      <td>P61</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4.519170</td>\n",
       "      <td>1.945910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  month  day  order  country  session_id  page1_main_category  \\\n",
       "78260   2008      7    4      2       29       17592                    3   \n",
       "5753    2008      6    1      6       29       12203                    2   \n",
       "60138   2008      4   15      5        9        3938                    1   \n",
       "115851  2008      7    6      3       29       17794                    1   \n",
       "84009   2008      6   18      7       29       15077                    4   \n",
       "\n",
       "       page2_clothing_model  colour  location  model_photography  price  \\\n",
       "78260                   C53      14         6                  1     38   \n",
       "5753                     B9       1         3                  1     48   \n",
       "60138                    A9       3         3                  1     82   \n",
       "115851                   A3       3         1                  1     72   \n",
       "84009                   P61       1         3                  2     28   \n",
       "\n",
       "        price_2  page  price_transformed  order_transformed  \n",
       "78260         2     3           5.080116           0.693147  \n",
       "5753          2     1           5.529871           1.791759  \n",
       "60138         1     1           6.632712           1.609438  \n",
       "115851        1     1           6.355335           1.098612  \n",
       "84009         2     4           4.519170           1.945910  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "556a457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# 2. Function to detect outliers using the IQR method (1.5 * IQR Rule)\n",
    "def detect_iqr_outliers(data, column, factor=1.5):\n",
    "    \"\"\"Detects outliers using the IQR method for a given continuous column.\"\"\"\n",
    "    \n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define the bounds\n",
    "    lower_bound = Q1 - (factor * IQR)\n",
    "    upper_bound = Q3 + (factor * IQR)\n",
    "    \n",
    "    # Identify the outliers\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    \n",
    "    # Report the findings\n",
    "    print(f\"\\n--- Outlier Analysis for '{column}' ---\")\n",
    "    print(f\"Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
    "    print(f\"Lower Bound: {lower_bound:.2f}\")\n",
    "    print(f\"Upper Bound: {upper_bound:.2f}\")\n",
    "    print(f\"Total Outliers Found: {len(outliers)}\")\n",
    "    \n",
    "    if not outliers.empty:\n",
    "        # Show the actual outlier values\n",
    "        print(f\"Outlier Values:\")\n",
    "        print(outliers[[column]])\n",
    "    else:\n",
    "        print(\"No outliers found.\")\n",
    "    \n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# 3. Define the continuous columns\n",
    "# continuous_features = ['Price', 'Area'] \n",
    "\n",
    "# 4. Loop through the continuous columns and apply the detection function\n",
    "# all_outliers = {}\n",
    "# for feature in continuous_features:\n",
    "#     outliers, lower_bound, upper_bound = detect_iqr_outliers(df, feature)\n",
    "#     all_outliers[feature] = {'outliers': outliers, 'lower': lower_bound, 'upper': upper_bound}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0bfc3dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Outlier Analysis for 'order' ---\n",
      "Q1: 2.00, Q3: 12.00, IQR: 10.00\n",
      "Lower Bound: -13.00\n",
      "Upper Bound: 27.00\n",
      "Total Outliers Found: 7308\n",
      "Outlier Values:\n",
      "        order\n",
      "98961      31\n",
      "128684     61\n",
      "71417      35\n",
      "97096      42\n",
      "83715      35\n",
      "...       ...\n",
      "52256      36\n",
      "38660      48\n",
      "106530     72\n",
      "119346     37\n",
      "5311       41\n",
      "\n",
      "[7308 rows x 1 columns]\n",
      "\n",
      "--- Outlier Analysis for 'price' ---\n",
      "Q1: 33.00, Q3: 52.00, IQR: 19.00\n",
      "Lower Bound: 4.50\n",
      "Upper Bound: 80.50\n",
      "Total Outliers Found: 1233\n",
      "Outlier Values:\n",
      "        price\n",
      "60138      82\n",
      "116171     82\n",
      "56288      82\n",
      "68443      82\n",
      "71952      82\n",
      "...       ...\n",
      "18070      82\n",
      "131926     82\n",
      "52733      82\n",
      "13545      82\n",
      "64925      82\n",
      "\n",
      "[1233 rows x 1 columns]\n",
      "\n",
      "--- Outlier Analysis for 'price_transformed' ---\n",
      "Q1: 4.82, Q3: 5.69, IQR: 0.87\n",
      "Lower Bound: 3.51\n",
      "Upper Bound: 6.99\n",
      "Total Outliers Found: 0\n",
      "No outliers found.\n",
      "\n",
      "--- Outlier Analysis for 'order_transformed' ---\n",
      "Q1: 0.69, Q3: 2.48, IQR: 1.79\n",
      "Lower Bound: -1.99\n",
      "Upper Bound: 5.17\n",
      "Total Outliers Found: 26\n",
      "Outlier Values:\n",
      "        order_transformed\n",
      "16790            5.176150\n",
      "68360            5.236442\n",
      "111344           5.247024\n",
      "102100           5.209486\n",
      "88128            5.252273\n",
      "98283            5.214936\n",
      "27266            5.181784\n",
      "9362             5.209486\n",
      "20883            5.204007\n",
      "48248            5.204007\n",
      "89470            5.214936\n",
      "62742            5.214936\n",
      "15266            5.252273\n",
      "92582            5.273000\n",
      "129602           5.181784\n",
      "6228             5.187386\n",
      "46456            5.176150\n",
      "115047           5.187386\n",
      "29350            5.231109\n",
      "56441            5.192957\n",
      "129899           5.225747\n",
      "45591            5.236442\n",
      "30782            5.181784\n",
      "107904           5.192957\n",
      "51356            5.241747\n",
      "79767            5.192957\n"
     ]
    }
   ],
   "source": [
    "# 4. Loop through the continuous columns and apply the detection function\n",
    "all_outliers = {}\n",
    "for feature in cont_col:\n",
    "    outliers, lower_bound, upper_bound = detect_iqr_outliers(df_train, feature)\n",
    "    all_outliers[feature] = {'outliers': outliers, 'lower': lower_bound, 'upper': upper_bound}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3706c90b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[116]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# import pandas as pd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# import numpy as np\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 3. Create a visualization to show outliers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GIT HUB\\GUVI Mini Proj 4\\clickstream_venv\\Lib\\site-packages\\matplotlib\\__init__.py:1299\u001b[39m\n\u001b[32m   1295\u001b[39m     rcParams[\u001b[33m'\u001b[39m\u001b[33mbackend_fallback\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.environ.get(\u001b[33m'\u001b[39m\u001b[33mMPLBACKEND\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1299\u001b[39m     \u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbackend\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m = os.environ.get(\u001b[33m'\u001b[39m\u001b[33mMPLBACKEND\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_backend\u001b[39m(*, auto_select=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1303\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1304\u001b[39m \u001b[33;03m    Return the name of the current backend.\u001b[39;00m\n\u001b[32m   1305\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1323\u001b[39m \u001b[33;03m    matplotlib.use\u001b[39;00m\n\u001b[32m   1324\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GIT HUB\\GUVI Mini Proj 4\\clickstream_venv\\Lib\\site-packages\\matplotlib\\__init__.py:772\u001b[39m, in \u001b[36mRcParams.__setitem__\u001b[39m\u001b[34m(self, key, val)\u001b[39m\n\u001b[32m    770\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m772\u001b[39m     cval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mve\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GIT HUB\\GUVI Mini Proj 4\\clickstream_venv\\Lib\\site-packages\\matplotlib\\rcsetup.py:273\u001b[39m, in \u001b[36mvalidate_backend\u001b[39m\u001b[34m(s)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_backend\u001b[39m(s):\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m _auto_backend_sentinel \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mbackend_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_valid_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    274\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m s\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GIT HUB\\GUVI Mini Proj 4\\clickstream_venv\\Lib\\site-packages\\matplotlib\\backends\\registry.py:244\u001b[39m, in \u001b[36mBackendRegistry.is_valid_backend\u001b[39m\u001b[34m(self, backend)\u001b[39m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# Only load entry points if really need to and not already done so.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_entry_points_loaded\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend_to_gui_framework:\n\u001b[32m    246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GIT HUB\\GUVI Mini Proj 4\\clickstream_venv\\Lib\\site-packages\\matplotlib\\backends\\registry.py:116\u001b[39m, in \u001b[36mBackendRegistry._ensure_entry_points_loaded\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_ensure_entry_points_loaded\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    114\u001b[39m     \u001b[38;5;66;03m# Load entry points, if they have not already been loaded.\u001b[39;00m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._loaded_entry_points:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         entries = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_entry_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m         \u001b[38;5;28mself\u001b[39m._validate_and_store_entry_points(entries)\n\u001b[32m    118\u001b[39m         \u001b[38;5;28mself\u001b[39m._loaded_entry_points = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GIT HUB\\GUVI Mini Proj 4\\clickstream_venv\\Lib\\site-packages\\matplotlib\\backends\\registry.py:136\u001b[39m, in \u001b[36mBackendRegistry._read_entry_points\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_entry_points\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    129\u001b[39m     \u001b[38;5;66;03m# Read entry points of modules that self-advertise as Matplotlib backends.\u001b[39;00m\n\u001b[32m    130\u001b[39m     \u001b[38;5;66;03m# Expects entry points like this one from matplotlib-inline (in pyproject.toml\u001b[39;00m\n\u001b[32m    131\u001b[39m     \u001b[38;5;66;03m# format):\u001b[39;00m\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m#   [project.entry-points.\"matplotlib.backend\"]\u001b[39;00m\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m#   inline = \"matplotlib_inline.backend_inline\"\u001b[39;00m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mim\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     entry_points = \u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mentry_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmatplotlib.backend\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m     entries = [(entry.name, entry.value) \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m entry_points]\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# For backward compatibility, if matplotlib-inline and/or ipympl are installed\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# but too old to include entry points, create them. Do not import ipympl\u001b[39;00m\n\u001b[32m    141\u001b[39m     \u001b[38;5;66;03m# directly as this calls matplotlib.use() whilst in this function.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\importlib\\metadata\\__init__.py:1040\u001b[39m, in \u001b[36mentry_points\u001b[39m\u001b[34m(**params)\u001b[39m\n\u001b[32m   1021\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return EntryPoint objects for all installed packages.\u001b[39;00m\n\u001b[32m   1022\u001b[39m \n\u001b[32m   1023\u001b[39m \u001b[33;03mPass selection parameters (group or name) to filter the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1035\u001b[39m \u001b[33;03m:return: EntryPoints or SelectableGroups for all installed packages.\u001b[39;00m\n\u001b[32m   1036\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1037\u001b[39m eps = itertools.chain.from_iterable(\n\u001b[32m   1038\u001b[39m     dist.entry_points \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m _unique(distributions())\n\u001b[32m   1039\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1040\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSelectableGroups\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m.select(**params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\importlib\\metadata\\__init__.py:476\u001b[39m, in \u001b[36mSelectableGroups.load\u001b[39m\u001b[34m(cls, eps)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, eps):\n\u001b[32m    475\u001b[39m     by_group = operator.attrgetter(\u001b[33m'\u001b[39m\u001b[33mgroup\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     ordered = \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    477\u001b[39m     grouped = itertools.groupby(ordered, by_group)\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m((group, EntryPoints(eps)) \u001b[38;5;28;01mfor\u001b[39;00m group, eps \u001b[38;5;129;01min\u001b[39;00m grouped)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\importlib\\metadata\\__init__.py:1038\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1020\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mentry_points\u001b[39m(**params) -> Union[EntryPoints, SelectableGroups]:\n\u001b[32m   1021\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return EntryPoint objects for all installed packages.\u001b[39;00m\n\u001b[32m   1022\u001b[39m \n\u001b[32m   1023\u001b[39m \u001b[33;03m    Pass selection parameters (group or name) to filter the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1035\u001b[39m \u001b[33;03m    :return: EntryPoints or SelectableGroups for all installed packages.\u001b[39;00m\n\u001b[32m   1036\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1037\u001b[39m     eps = itertools.chain.from_iterable(\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m         \u001b[43mdist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mentry_points\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m _unique(distributions())\n\u001b[32m   1039\u001b[39m     )\n\u001b[32m   1040\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SelectableGroups.load(eps).select(**params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\importlib\\metadata\\__init__.py:636\u001b[39m, in \u001b[36mDistribution.entry_points\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mentry_points\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m EntryPoints._from_text_for(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mentry_points.txt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\importlib\\metadata\\__init__.py:938\u001b[39m, in \u001b[36mPathDistribution.read_text\u001b[39m\u001b[34m(self, filename)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\n\u001b[32m    932\u001b[39m         \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m,\n\u001b[32m    933\u001b[39m         \u001b[38;5;167;01mIsADirectoryError\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    936\u001b[39m         \u001b[38;5;167;01mPermissionError\u001b[39;00m,\n\u001b[32m    937\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m938\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoinpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\pathlib.py:1058\u001b[39m, in \u001b[36mPath.read_text\u001b[39m\u001b[34m(self, encoding, errors)\u001b[39m\n\u001b[32m   1054\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[33;03mOpen the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1057\u001b[39m encoding = io.text_encoding(encoding)\n\u001b[32m-> \u001b[39m\u001b[32m1058\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   1059\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\pathlib.py:1044\u001b[39m, in \u001b[36mPath.open\u001b[39m\u001b[34m(self, mode, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1043\u001b[39m     encoding = io.text_encoding(encoding)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m io.open(\u001b[38;5;28mself\u001b[39m, mode, buffering, encoding, errors, newline)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:309\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, errors)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# 3. Create a visualization to show outliers\n",
    "fig, axes = plt.subplots(4, 2, figsize=(14, 10))\n",
    "fig.suptitle('Outlier Visualization for Continuous Features', fontsize=16)\n",
    "plot_index = 0\n",
    "\n",
    "for feature in cont_col:\n",
    "    # A. Box Plot: Best visualization for the IQR method\n",
    "    sns.boxplot(y=df_train[feature], ax=axes[plot_index, 0], color='skyblue')\n",
    "    axes[plot_index, 0].set_title(f'Box Plot of {feature}', fontsize=12)\n",
    "    axes[plot_index, 0].set_ylabel(feature)\n",
    "\n",
    "    # B. Histogram / Distribution Plot: Shows overall data spread\n",
    "    sns.histplot(df_train[feature], kde=True, ax=axes[plot_index, 1], color='salmon')\n",
    "    axes[plot_index, 1].set_title(f'Distribution of {feature}', fontsize=12)\n",
    "    axes[plot_index, 1].set_xlabel(feature)\n",
    "    \n",
    "    plot_index += 1\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# plt.savefig('continuous_feature_outliers_visualization.png')\n",
    "plt.show() # Note: This line is removed for notebook execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71ba406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winsorizing /Capping feature to upper bound\n",
    "df_train['order_capped'] = df_train['order_transformed'].apply(lambda i : 5.17 if i > 5.17 else i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aaf1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Outlier Analysis for 'order_capped' ---\n",
      "Q1: 0.69, Q3: 2.48, IQR: 1.79\n",
      "Lower Bound: -1.99\n",
      "Upper Bound: 5.17\n",
      "Total Outliers Found: 0\n",
      "No outliers found.\n"
     ]
    }
   ],
   "source": [
    "# 4. Loop through the continuous columns and apply the detection function\n",
    "all_outliers = {}\n",
    "outliers, lower_bound, upper_bound = detect_iqr_outliers(df_train,'order_capped' )\n",
    "all_outliers[feature] = {'outliers': outliers, 'lower': lower_bound, 'upper': upper_bound}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a06f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The IQR method is generally the better and safer choice for initial outlier detection in real-world data \n",
    "# because it is more robust and less susceptible to the masking effect. \n",
    "# The Z-score method is best reserved for data that is known to follow a Normal (Gaussian) distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e59e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Z-Score Outlier Analysis for 'order' (Threshold: |Z| > 3) ---\n",
      "Total Outliers Found: 2064\n",
      "Outlier Values and Z-Scores:\n",
      "        order    Z_Score\n",
      "92582     195  13.843428\n",
      "15266     191  13.544445\n",
      "88128     191  13.544445\n",
      "111344    190  13.469699\n",
      "51356     189  13.394953\n",
      "...       ...        ...\n",
      "93625      50   3.005302\n",
      "125563     50   3.005302\n",
      "18628      50   3.005302\n",
      "82924      50   3.005302\n",
      "9494       50   3.005302\n",
      "\n",
      "[2064 rows x 2 columns]\n",
      "\n",
      "--- Z-Score Outlier Analysis for 'price' (Threshold: |Z| > 3) ---\n",
      "Total Outliers Found: 1233\n",
      "Outlier Values and Z-Scores:\n",
      "        price   Z_Score\n",
      "60138      82  3.050754\n",
      "116171     82  3.050754\n",
      "56288      82  3.050754\n",
      "68443      82  3.050754\n",
      "71952      82  3.050754\n",
      "...       ...       ...\n",
      "18070      82  3.050754\n",
      "131926     82  3.050754\n",
      "52733      82  3.050754\n",
      "13545      82  3.050754\n",
      "64925      82  3.050754\n",
      "\n",
      "[1233 rows x 2 columns]\n",
      "\n",
      "--- Z-Score Outlier Analysis for 'price_transformed' (Threshold: |Z| > 3) ---\n",
      "Total Outliers Found: 0\n",
      "No outliers found.\n",
      "\n",
      "--- Z-Score Outlier Analysis for 'order_transformed' (Threshold: |Z| > 3) ---\n",
      "Total Outliers Found: 106\n",
      "Outlier Values and Z-Scores:\n",
      "        order_transformed   Z_Score\n",
      "92582            5.273000  3.299707\n",
      "88128            5.252273  3.280598\n",
      "15266            5.252273  3.280598\n",
      "111344           5.247024  3.275758\n",
      "51356            5.241747  3.270892\n",
      "...                   ...       ...\n",
      "52178            4.962845  3.013738\n",
      "24050            4.955827  3.007268\n",
      "102928           4.955827  3.007268\n",
      "34315            4.948760  3.000752\n",
      "28369            4.948760  3.000752\n",
      "\n",
      "[106 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import the zscore function from SciPy\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# 2. Function to detect outliers using the Z-score method\n",
    "def detect_zscore_outliers(data, column, threshold=3):\n",
    "    \"\"\"\n",
    "    Detects outliers using the Z-score method (|Z| > threshold).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate Z-scores for the column\n",
    "    # The zscore function automatically handles mean and standard deviation calculation\n",
    "    z_scores = zscore(data[column])\n",
    "    \n",
    "    # Create a boolean mask where absolute Z-score is greater than the threshold\n",
    "    outliers_mask = np.abs(z_scores) > threshold\n",
    "    \n",
    "    # Filter the original data using the mask\n",
    "    outliers = data[outliers_mask]\n",
    "    \n",
    "    # Report the findings\n",
    "    print(f\"\\n--- Z-Score Outlier Analysis for '{column}' (Threshold: |Z| > {threshold}) ---\")\n",
    "    print(f\"Total Outliers Found: {len(outliers)}\")\n",
    "    \n",
    "    if not outliers.empty:\n",
    "        # Show the actual outlier values and their calculated Z-scores\n",
    "        outlier_data = outliers.copy()\n",
    "        # Add Z-Score back to the outlier dataframe for easy viewing\n",
    "        outlier_data[f'Z_Score'] = z_scores[outliers_mask]\n",
    "        print(f\"Outlier Values and Z-Scores:\")\n",
    "        print(outlier_data[[column, f'Z_Score']].sort_values(by=f'Z_Score', ascending=False))\n",
    "    else:\n",
    "        print(\"No outliers found.\")\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# # 3. Define the continuous columns and run the analysis\n",
    "# continuous_features = ['Price', 'Area'] \n",
    "\n",
    "for feature in cont_col:\n",
    "    detect_zscore_outliers(df_train, feature, threshold=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77268d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Z-Score Outlier Analysis for 'order_capped' (Threshold: |Z| > 3) ---\n",
      "Total Outliers Found: 106\n",
      "Outlier Values and Z-Scores:\n",
      "        order_capped   Z_Score\n",
      "68360       5.170000  3.204849\n",
      "16790       5.170000  3.204849\n",
      "9362        5.170000  3.204849\n",
      "102100      5.170000  3.204849\n",
      "88128       5.170000  3.204849\n",
      "...              ...       ...\n",
      "52178       4.962845  3.013841\n",
      "24050       4.955827  3.007370\n",
      "102928      4.955827  3.007370\n",
      "34315       4.948760  3.000854\n",
      "28369       4.948760  3.000854\n",
      "\n",
      "[106 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>order</th>\n",
       "      <th>country</th>\n",
       "      <th>session_id</th>\n",
       "      <th>page1_main_category</th>\n",
       "      <th>page2_clothing_model</th>\n",
       "      <th>colour</th>\n",
       "      <th>location</th>\n",
       "      <th>model_photography</th>\n",
       "      <th>price</th>\n",
       "      <th>price_2</th>\n",
       "      <th>page</th>\n",
       "      <th>price_transformed</th>\n",
       "      <th>order_transformed</th>\n",
       "      <th>order_capped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61809</th>\n",
       "      <td>2008</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>148</td>\n",
       "      <td>9</td>\n",
       "      <td>13846</td>\n",
       "      <td>3</td>\n",
       "      <td>C28</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5.529871</td>\n",
       "      <td>4.997212</td>\n",
       "      <td>4.997212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26682</th>\n",
       "      <td>2008</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>144</td>\n",
       "      <td>29</td>\n",
       "      <td>22433</td>\n",
       "      <td>1</td>\n",
       "      <td>A28</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.315813</td>\n",
       "      <td>4.969813</td>\n",
       "      <td>4.969813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16790</th>\n",
       "      <td>2008</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>177</td>\n",
       "      <td>9</td>\n",
       "      <td>13846</td>\n",
       "      <td>3</td>\n",
       "      <td>C49</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.387490</td>\n",
       "      <td>5.176150</td>\n",
       "      <td>5.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111123</th>\n",
       "      <td>2008</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>151</td>\n",
       "      <td>29</td>\n",
       "      <td>22433</td>\n",
       "      <td>1</td>\n",
       "      <td>A41</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5.315813</td>\n",
       "      <td>5.017280</td>\n",
       "      <td>5.017280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33404</th>\n",
       "      <td>2008</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>158</td>\n",
       "      <td>29</td>\n",
       "      <td>22902</td>\n",
       "      <td>1</td>\n",
       "      <td>A16</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.817257</td>\n",
       "      <td>5.062595</td>\n",
       "      <td>5.062595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102928</th>\n",
       "      <td>2008</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>142</td>\n",
       "      <td>9</td>\n",
       "      <td>13846</td>\n",
       "      <td>3</td>\n",
       "      <td>C24</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4.817257</td>\n",
       "      <td>4.955827</td>\n",
       "      <td>4.955827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107904</th>\n",
       "      <td>2008</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>180</td>\n",
       "      <td>29</td>\n",
       "      <td>22433</td>\n",
       "      <td>3</td>\n",
       "      <td>C45</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5.800530</td>\n",
       "      <td>5.192957</td>\n",
       "      <td>5.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120674</th>\n",
       "      <td>2008</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>169</td>\n",
       "      <td>29</td>\n",
       "      <td>22902</td>\n",
       "      <td>1</td>\n",
       "      <td>A26</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.080116</td>\n",
       "      <td>5.129899</td>\n",
       "      <td>5.129899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51356</th>\n",
       "      <td>2008</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>189</td>\n",
       "      <td>9</td>\n",
       "      <td>13846</td>\n",
       "      <td>3</td>\n",
       "      <td>C56</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5.872616</td>\n",
       "      <td>5.241747</td>\n",
       "      <td>5.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79767</th>\n",
       "      <td>2008</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>180</td>\n",
       "      <td>29</td>\n",
       "      <td>22902</td>\n",
       "      <td>1</td>\n",
       "      <td>A37</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6.044095</td>\n",
       "      <td>5.192957</td>\n",
       "      <td>5.170000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  month  day  order  country  session_id  page1_main_category  \\\n",
       "61809   2008      6   10    148        9       13846                    3   \n",
       "26682   2008      8    3    144       29       22433                    1   \n",
       "16790   2008      6   10    177        9       13846                    3   \n",
       "111123  2008      8    3    151       29       22433                    1   \n",
       "33404   2008      8    6    158       29       22902                    1   \n",
       "...      ...    ...  ...    ...      ...         ...                  ...   \n",
       "102928  2008      6   10    142        9       13846                    3   \n",
       "107904  2008      8    3    180       29       22433                    3   \n",
       "120674  2008      8    6    169       29       22902                    1   \n",
       "51356   2008      6   10    189        9       13846                    3   \n",
       "79767   2008      8    6    180       29       22902                    1   \n",
       "\n",
       "       page2_clothing_model  colour  location  model_photography  price  \\\n",
       "61809                   C28       2         4                  2     48   \n",
       "26682                   A28       1         4                  2     43   \n",
       "16790                   C49       4         5                  2     26   \n",
       "111123                  A41       4         2                  1     43   \n",
       "33404                   A16       1         6                  1     33   \n",
       "...                     ...     ...       ...                ...    ...   \n",
       "102928                  C24      12         2                  1     33   \n",
       "107904                  C45      11         3                  1     55   \n",
       "120674                  A26       2         3                  1     38   \n",
       "51356                   C56       6         1                  2     57   \n",
       "79767                   A37       2         1                  1     62   \n",
       "\n",
       "        price_2  page  price_transformed  order_transformed  order_capped  \n",
       "61809         1     2           5.529871           4.997212      4.997212  \n",
       "26682         2     2           5.315813           4.969813      4.969813  \n",
       "16790         2     3           4.387490           5.176150      5.170000  \n",
       "111123        2     3           5.315813           5.017280      5.017280  \n",
       "33404         2     1           4.817257           5.062595      5.062595  \n",
       "...         ...   ...                ...                ...           ...  \n",
       "102928        2     2           4.817257           4.955827      4.955827  \n",
       "107904        1     3           5.800530           5.192957      5.170000  \n",
       "120674        2     2           5.080116           5.129899      5.129899  \n",
       "51356         1     4           5.872616           5.241747      5.170000  \n",
       "79767         1     3           6.044095           5.192957      5.170000  \n",
       "\n",
       "[106 rows x 17 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_zscore_outliers(df_train, \"order_capped\", threshold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0944be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-encoding\n",
    "# split the data into test and train and perform all these actions in train - step 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd89e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant features\n",
    "# ['price', --- not req - done\n",
    "#  'price_transformed', --- cont\n",
    "#  'month',5 --- OHE\n",
    "#  'country',46 --- grouped and OHE  - done\n",
    "#  'session_id', --- unique (not req) - to be removed-----------------------------------------\n",
    "#  'page2_clothing_model',216 --- Target encode - done\n",
    "#  'page1_main_category',4 --- OHE\n",
    "#  'colour',14 --- OHE to be grouped and OHE\n",
    "#  'location',6 --- OHE \n",
    "#  'model_photography',2 --- OHE - label(binary) with 0/1\n",
    "#  'page', --- label(ordinal) - already done\n",
    "#  'model_encoded'] --- already done a frequecny ----to be removed-------------------------------\n",
    "# ohe=['month','page1_main_category','colour','location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d431c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # For demonstration, we'll re-create the DataFrame with high cardinality\n",
    "# # In your real environment, you would load your existing 'df'.\n",
    "# np.random.seed(42)\n",
    "# N = 1000 \n",
    "# countries_list = ['USA'] * 300 + ['Germany'] * 250 + ['UK'] * 150 + ['France'] * 100 \n",
    "# num_rare_countries = 43\n",
    "# rare_country_data = [f'Country_{i}' for i in range(num_rare_countries) for _ in range(5)]\n",
    "# countries_list += rare_country_data \n",
    "# countries_list = countries_list[:N] \n",
    "# np.random.shuffle(countries_list)\n",
    "\n",
    "# models_list = ['Model_A'] * 100 + ['Model_B'] * 80 + ['Model_C'] * 60\n",
    "# num_rare_models = 213\n",
    "# rare_model_data = [f'Model_{i}' for i in range(num_rare_models) for _ in range(4)]\n",
    "# models_list += rare_model_data\n",
    "# models_list = models_list[:N]\n",
    "# np.random.shuffle(models_list)\n",
    "\n",
    "# df = pd.DataFrame({\n",
    "#     'country': countries_list,\n",
    "#     'page2_clothing_model': models_list,\n",
    "#     'target': np.random.randint(0, 2, size=N) \n",
    "# })\n",
    "\n",
    "# --- Main Grouping Function ---\n",
    "def group_rare_categories(df, column_name, min_percentage=0.01, rare_label='RARE_GROUP'):\n",
    "    \"\"\"\n",
    "    Groups categories in a specified column that fall below a minimum\n",
    "    percentage threshold into a single 'RARE_GROUP' and returns the updated DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - column_name (str): The name of the categorical column to process.\n",
    "    - min_percentage (float): The threshold (e.g., 0.01 for 1%).\n",
    "    - rare_label (str): The label to use for the grouped rare categories.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Calculate the normalized frequency counts\n",
    "    value_counts = df[column_name].value_counts(normalize=True)\n",
    "    \n",
    "    # 2. Identify the categories to be grouped\n",
    "    rare_categories = value_counts[value_counts < min_percentage].index.tolist()\n",
    "    \n",
    "    # Check if grouping is necessary\n",
    "    if not rare_categories:\n",
    "        print(f\"No categories in '{column_name}' were below the {min_percentage*100:.2f}% threshold. Skipping.\")\n",
    "        return df\n",
    "    \n",
    "    # 3. Create the new column name\n",
    "    new_column_name = f'{column_name}_grouped'\n",
    "    df[new_column_name] = df[column_name].copy()\n",
    "    \n",
    "    # 4. Apply the grouping using .loc for efficient assignment\n",
    "    df.loc[df[column_name].isin(rare_categories), new_column_name] = rare_label\n",
    "    \n",
    "    # 5. Report summary\n",
    "    original_cardinality = df[column_name].nunique()\n",
    "    new_cardinality = df[new_column_name].nunique()\n",
    "    \n",
    "    print(f\"\\n--- Grouping Summary for '{column_name}' (Threshold: {min_percentage*100:.2f}%) ---\")\n",
    "    print(f\"Original Cardinality: {original_cardinality}\")\n",
    "    print(f\"Categories Grouped into '{rare_label}': {len(rare_categories)}\")\n",
    "    print(f\"New Cardinality: {new_cardinality}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f80050e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Grouping Summary for 'country' (Threshold: 1.00%) ---\n",
      "Original Cardinality: 44\n",
      "Categories Grouped into 'RARE_GROUP': 40\n",
      "New Cardinality: 5\n",
      "\n",
      "--- Grouping Summary for 'page2_clothing_model' (Threshold: 0.50%) ---\n",
      "Original Cardinality: 216\n",
      "Categories Grouped into 'RARE_GROUP': 145\n",
      "New Cardinality: 72\n",
      "\n",
      "--- Final Grouped Data Head ---\n",
      "        country country_grouped page2_clothing_model  \\\n",
      "78260        29              29                  C53   \n",
      "5753         29              29                   B9   \n",
      "60138         9               9                   A9   \n",
      "115851       29              29                   A3   \n",
      "84009        29              29                  P61   \n",
      "\n",
      "       page2_clothing_model_grouped  \n",
      "78260                    RARE_GROUP  \n",
      "5753                             B9  \n",
      "60138                            A9  \n",
      "115851                           A3  \n",
      "84009                    RARE_GROUP  \n",
      "\n",
      "Final Value Counts for 'country_grouped':\n",
      "country_grouped\n",
      "29            85683\n",
      "9             11615\n",
      "RARE_GROUP     4417\n",
      "24             2620\n",
      "46             1568\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final Value Counts for 'page2_clothing_model_grouped':\n",
      "page2_clothing_model_grouped\n",
      "RARE_GROUP    39797\n",
      "B4             2256\n",
      "A2             1907\n",
      "A11            1795\n",
      "P1             1717\n",
      "              ...  \n",
      "C11             545\n",
      "P48             544\n",
      "P33             543\n",
      "B21             536\n",
      "C50             531\n",
      "Name: count, Length: 72, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_19820\\3883849262.py:57: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'RARE_GROUP' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df[column_name].isin(rare_categories), new_column_name] = rare_label\n"
     ]
    }
   ],
   "source": [
    "# A. Grouping 'country' (Original Card: 46)\n",
    "# Using a 1% threshold\n",
    "df = group_rare_categories(df_train, 'country', min_percentage=0.01)\n",
    "\n",
    "# B. Grouping 'page2_clothing_model' (Original Card: 216)\n",
    "# Using a slightly lower 0.5% threshold to ensure more rare models are grouped\n",
    "df = group_rare_categories(df_train, 'page2_clothing_model', min_percentage=0.005)\n",
    "\n",
    "# 3. Final Verification of the New Columns\n",
    "print(\"\\n--- Final Grouped Data Head ---\")\n",
    "print(df[['country', 'country_grouped', 'page2_clothing_model', 'page2_clothing_model_grouped']].head())\n",
    "print(\"\\nFinal Value Counts for 'country_grouped':\")\n",
    "print(df['country_grouped'].value_counts())\n",
    "print(\"\\nFinal Value Counts for 'page2_clothing_model_grouped':\")\n",
    "print(df['page2_clothing_model_grouped'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1868b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a0daa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Encoded Features Head ---\n",
      "\n",
      "Target Encoding (model_target_encoded) is complete and leakage-free.\n",
      "One-Hot Encoding (country_...) is complete and safe.\n"
     ]
    }
   ],
   "source": [
    "# target encoding and OHE\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assume 'df' is your DataFrame with 'country_grouped', 'page2_clothing_model_grouped', and 'target'\n",
    "# (The 'target' is your classification outcome, e.g., 'purchased' or 'converted')\n",
    "\n",
    "# --- 1. Target Encoding Setup for 'page2_clothing_model_grouped' ---\n",
    "\n",
    "# We will use StratifiedKFold to ensure the target ratio is preserved in each fold\n",
    "NFOLDS = 5\n",
    "kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a placeholder column for the encoded feature\n",
    "df_train['model_target_encoded'] = np.nan\n",
    "\n",
    "# Perform Target Encoding via K-Fold Cross-Validation\n",
    "for train_index, val_index in kf.split(df_train, df_train['price_2']):\n",
    "    # Initialize TargetEncoder (smoothing is default and recommended)\n",
    "    encoder = TargetEncoder()\n",
    "    \n",
    "    # Fit the encoder ONLY on the K-1 folds (train data)\n",
    "    encoder.fit(df_train.iloc[train_index]['page2_clothing_model_grouped'], df_train.iloc[train_index]['price_2'])\n",
    "    \n",
    "    # Transform the remaining fold (validation data)\n",
    "    # The transformation uses the means calculated from the train_index\n",
    "    df_train.loc[val_index, 'model_target_encoded'] = encoder.transform(\n",
    "        df_train.iloc[val_index]['page2_clothing_model_grouped']\n",
    "    ).to_numpy().flatten()\n",
    "\n",
    "# For any new data (when predicting), you must fit the encoder on the ENTIRE training set.\n",
    "final_model_encoder = TargetEncoder()\n",
    "final_model_encoder.fit(df_train['page2_clothing_model_grouped'], df_train['price_2'])\n",
    "\n",
    "\n",
    "# --- 2. One-Hot Encoding for 'country_grouped' (The Simple, Safe Choice) ---\n",
    "\n",
    "# Use pandas get_dummies for simple OHE\n",
    "country_ohe = pd.get_dummies(df_train['country_grouped'], prefix='country', dtype=int)\n",
    "\n",
    "# Concatenate the new OHE columns to the main DataFrame\n",
    "df_train = pd.concat([df_train.drop('country_grouped', axis=1), country_ohe], axis=1)\n",
    "\n",
    "# Display the result (replace 'page2_clothing_model_grouped' with the encoded column)\n",
    "print(\"\\n--- Final Encoded Features Head ---\")\n",
    "# print(data[['country_RARE_GROUP','model_target_encoded']].head())\n",
    "\n",
    "print(\"\\nTarget Encoding (model_target_encoded) is complete and leakage-free.\")\n",
    "print(\"One-Hot Encoding (country_...) is complete and safe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475858ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn and understand the target encoding more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8300bbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>order</th>\n",
       "      <th>country</th>\n",
       "      <th>session_id</th>\n",
       "      <th>page1_main_category</th>\n",
       "      <th>page2_clothing_model</th>\n",
       "      <th>colour</th>\n",
       "      <th>location</th>\n",
       "      <th>...</th>\n",
       "      <th>price_transformed</th>\n",
       "      <th>order_transformed</th>\n",
       "      <th>order_capped</th>\n",
       "      <th>page2_clothing_model_grouped</th>\n",
       "      <th>model_target_encoded</th>\n",
       "      <th>country_9</th>\n",
       "      <th>country_24</th>\n",
       "      <th>country_29</th>\n",
       "      <th>country_46</th>\n",
       "      <th>country_RARE_GROUP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105898</th>\n",
       "      <td>2008</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "      <td>12417</td>\n",
       "      <td>4</td>\n",
       "      <td>P21</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4.817257</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>RARE_GROUP</td>\n",
       "      <td>1.545677</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105899</th>\n",
       "      <td>2008</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>7938</td>\n",
       "      <td>2</td>\n",
       "      <td>B17</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>5.080116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>B17</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105900</th>\n",
       "      <td>2008</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>16249</td>\n",
       "      <td>4</td>\n",
       "      <td>P3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.529871</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>P3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105901</th>\n",
       "      <td>2008</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>13674</td>\n",
       "      <td>4</td>\n",
       "      <td>P16</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>4.817257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>P16</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105902</th>\n",
       "      <td>2008</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>21622</td>\n",
       "      <td>1</td>\n",
       "      <td>A7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>5.315813</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>A7</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  month  day  order  country  session_id  page1_main_category  \\\n",
       "105898  2008      6    2     15       29       12417                    4   \n",
       "105899  2008      5    7      1       29        7938                    2   \n",
       "105900  2008      6   26      3       25       16249                    4   \n",
       "105901  2008      6    9      1       29       13674                    4   \n",
       "105902  2008      7   29      4       29       21622                    1   \n",
       "\n",
       "       page2_clothing_model  colour  location  ...  price_transformed  \\\n",
       "105898                  P21       4         1  ...           4.817257   \n",
       "105899                  B17       6         6  ...           5.080116   \n",
       "105900                   P3       2         1  ...           5.529871   \n",
       "105901                  P16       7         6  ...           4.817257   \n",
       "105902                   A7       3         3  ...           5.315813   \n",
       "\n",
       "        order_transformed  order_capped  page2_clothing_model_grouped  \\\n",
       "105898           2.708050      2.708050                    RARE_GROUP   \n",
       "105899           0.000000      0.000000                           B17   \n",
       "105900           1.098612      1.098612                            P3   \n",
       "105901           0.000000      0.000000                           P16   \n",
       "105902           1.386294      1.386294                            A7   \n",
       "\n",
       "        model_target_encoded  country_9  country_24 country_29  country_46  \\\n",
       "105898              1.545677          0           0          1           0   \n",
       "105899              2.000000          0           0          1           0   \n",
       "105900              1.000000          0           0          0           0   \n",
       "105901              2.000000          0           0          1           0   \n",
       "105902              2.000000          0           0          1           0   \n",
       "\n",
       "        country_RARE_GROUP  \n",
       "105898                   0  \n",
       "105899                   0  \n",
       "105900                   1  \n",
       "105901                   0  \n",
       "105902                   0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a0180b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Grouping Summary for 'colour' (Threshold: 1.00%) ---\n",
      "Original Cardinality: 14\n",
      "Categories Grouped into 'RARE_GROUP': 1\n",
      "New Cardinality: 14\n",
      "\n",
      "Final Value Counts for 'colour':\n",
      "colour\n",
      "2     19026\n",
      "3     18710\n",
      "6     11253\n",
      "4     10599\n",
      "14    10217\n",
      "9      8697\n",
      "12     5628\n",
      "1      5052\n",
      "7      4322\n",
      "13     4059\n",
      "11     3303\n",
      "10     2256\n",
      "8      1727\n",
      "5      1054\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_11292\\3883849262.py:57: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'RARE_GROUP' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df[column_name].isin(rare_categories), new_column_name] = rare_label\n"
     ]
    }
   ],
   "source": [
    "df = group_rare_categories(df_train, 'colour', min_percentage=0.01)\n",
    "print(\"\\nFinal Value Counts for 'colour':\")\n",
    "print(df['colour'].value_counts())\n",
    "# this can be removed since the cardinality is not changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e17176",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns='colour_grouped', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3584c0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding (month_...) is complete and safe.\n",
      "One-Hot Encoding (page1_main_category_...) is complete and safe.\n",
      "One-Hot Encoding (colour_...) is complete and safe.\n",
      "One-Hot Encoding (location_...) is complete and safe.\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding\n",
    "ohe=['month','page1_main_category','colour','location']\n",
    "for i in ohe:\n",
    "    # Use pandas get_dummies for simple OHE\n",
    "    df_ohe = pd.get_dummies(df_train[i], prefix=i, dtype=int)\n",
    "\n",
    "    # Concatenate the new OHE columns to the main DataFrame\n",
    "    df_train = pd.concat([df_train.drop(i, axis=1), df_ohe], axis=1)\n",
    "\n",
    "    # Display the result (replace 'page2_clothing_model_grouped' with the encoded column)\n",
    "    # print(\"\\n--- Final Encoded Features Head ---\")\n",
    "    # print(data[['country_RARE_GROUP','model_target_encoded']].head())\n",
    "\n",
    "    \n",
    "    print(f\"One-Hot Encoding ({i}_...) is complete and safe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edcceab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding\n",
    "df_train['model_photography'] = df_train['model_photography'].map({1:0, 2:1})\n",
    "# df_train['model_photography'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef42164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105903 entries, 0 to 105902\n",
      "Data columns (total 49 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   year                          105903 non-null  int64  \n",
      " 1   day                           105903 non-null  int64  \n",
      " 2   order                         105903 non-null  int64  \n",
      " 3   country                       105903 non-null  int64  \n",
      " 4   session_id                    105903 non-null  int64  \n",
      " 5   page2_clothing_model          105903 non-null  object \n",
      " 6   model_photography             105903 non-null  int64  \n",
      " 7   price                         105903 non-null  int64  \n",
      " 8   price_2                       105903 non-null  int64  \n",
      " 9   page                          105903 non-null  int64  \n",
      " 10  price_transformed             105903 non-null  float64\n",
      " 11  order_transformed             105903 non-null  float64\n",
      " 12  order_capped                  105903 non-null  float64\n",
      " 13  page2_clothing_model_grouped  105903 non-null  object \n",
      " 14  model_target_encoded          105903 non-null  float64\n",
      " 15  country_9                     105903 non-null  int64  \n",
      " 16  country_24                    105903 non-null  int64  \n",
      " 17  country_29                    105903 non-null  int64  \n",
      " 18  country_46                    105903 non-null  int64  \n",
      " 19  country_RARE_GROUP            105903 non-null  int64  \n",
      " 20  month_4                       105903 non-null  int64  \n",
      " 21  month_5                       105903 non-null  int64  \n",
      " 22  month_6                       105903 non-null  int64  \n",
      " 23  month_7                       105903 non-null  int64  \n",
      " 24  month_8                       105903 non-null  int64  \n",
      " 25  page1_main_category_1         105903 non-null  int64  \n",
      " 26  page1_main_category_2         105903 non-null  int64  \n",
      " 27  page1_main_category_3         105903 non-null  int64  \n",
      " 28  page1_main_category_4         105903 non-null  int64  \n",
      " 29  colour_1                      105903 non-null  int64  \n",
      " 30  colour_2                      105903 non-null  int64  \n",
      " 31  colour_3                      105903 non-null  int64  \n",
      " 32  colour_4                      105903 non-null  int64  \n",
      " 33  colour_5                      105903 non-null  int64  \n",
      " 34  colour_6                      105903 non-null  int64  \n",
      " 35  colour_7                      105903 non-null  int64  \n",
      " 36  colour_8                      105903 non-null  int64  \n",
      " 37  colour_9                      105903 non-null  int64  \n",
      " 38  colour_10                     105903 non-null  int64  \n",
      " 39  colour_11                     105903 non-null  int64  \n",
      " 40  colour_12                     105903 non-null  int64  \n",
      " 41  colour_13                     105903 non-null  int64  \n",
      " 42  colour_14                     105903 non-null  int64  \n",
      " 43  location_1                    105903 non-null  int64  \n",
      " 44  location_2                    105903 non-null  int64  \n",
      " 45  location_3                    105903 non-null  int64  \n",
      " 46  location_4                    105903 non-null  int64  \n",
      " 47  location_5                    105903 non-null  int64  \n",
      " 48  location_6                    105903 non-null  int64  \n",
      "dtypes: float64(4), int64(43), object(2)\n",
      "memory usage: 39.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deccdfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train steps ends here\n",
    "\n",
    "# df_test steps:\n",
    "# use the same OHE \n",
    "# Use the same label encoding\n",
    "# use the same grouping that is achieved from the train data set\n",
    "# Use the same values achieved from target encoding the train data set - on the test dataset\n",
    "# Also perform same transformation methods that is used on train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08ce05ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RARE_GROUP', 'B9', 'A9', 'A3', 'P1', 'C56', 'A31', 'C17', 'C49',\n",
       "       'B31', 'A12', 'C50', 'C12', 'B16', 'A8', 'A5', 'A14', 'C40', 'A17',\n",
       "       'P16', 'A21', 'A11', 'A7', 'A15', 'P17', 'P2', 'A4', 'B1', 'P33',\n",
       "       'P15', 'B23', 'C1', 'B12', 'B13', 'B4', 'P48', 'A16', 'B30', 'P4',\n",
       "       'P12', 'A18', 'P23', 'B10', 'A10', 'A2', 'B17', 'C14', 'B11',\n",
       "       'A13', 'A1', 'A6', 'C5', 'A33', 'C9', 'B15', 'C7', 'C2', 'B26',\n",
       "       'B3', 'B32', 'C11', 'P3', 'C8', 'B27', 'P6', 'B2', 'C13', 'B24',\n",
       "       'B14', 'B19', 'C4', 'B21'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target encoding on test data using the train data set\n",
    "df_train['page2_clothing_model_grouped'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49d94270",
   "metadata": {},
   "outputs": [],
   "source": [
    "page2_groups = ['RARE_GROUP', 'B9', 'A9', 'A3', 'P1', 'C56', 'A31', 'C17', 'C49',\n",
    "       'B31', 'A12', 'C50', 'C12', 'B16', 'A8', 'A5', 'A14', 'C40', 'A17',\n",
    "       'P16', 'A21', 'A11', 'A7', 'A15', 'P17', 'P2', 'A4', 'B1', 'P33',\n",
    "       'P15', 'B23', 'C1', 'B12', 'B13', 'B4', 'P48', 'A16', 'B30', 'P4',\n",
    "       'P12', 'A18', 'P23', 'B10', 'A10', 'A2', 'B17', 'C14', 'B11',\n",
    "       'A13', 'A1', 'A6', 'C5', 'A33', 'C9', 'B15', 'C7', 'C2', 'B26',\n",
    "       'B3', 'B32', 'C11', 'P3', 'C8', 'B27', 'P6', 'B2', 'C13', 'B24',\n",
    "       'B14', 'B19', 'C4', 'B21']\n",
    "df_test['page2_clothing_model_grouped'] = df_test['page2_clothing_model'].apply(lambda x: 'RARE_GROUP' if x not in page2_groups else x)\n",
    "# df_test['page2_clothing_model_grouped'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb3441ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_train[['page2_clothing_model_grouped','model_target_encoded']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "695ceb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_19820\\3221465772.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_temp.drop_duplicates(inplace= True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RARE_GROUP': 1.5457022929457462,\n",
       " 'B9': 2.0,\n",
       " 'A9': 1.0,\n",
       " 'A3': 1.0,\n",
       " 'P1': 1.0,\n",
       " 'C56': 1.0,\n",
       " 'A31': 1.0,\n",
       " 'C17': 1.0,\n",
       " 'C49': 2.0,\n",
       " 'B31': 1.0,\n",
       " 'A12': 2.0,\n",
       " 'C50': 2.0,\n",
       " 'C12': 1.0,\n",
       " 'B16': 1.0,\n",
       " 'A8': 1.0,\n",
       " 'A5': 2.0,\n",
       " 'A14': 1.0,\n",
       " 'C40': 2.0,\n",
       " 'A17': 1.0,\n",
       " 'P16': 2.0,\n",
       " 'A21': 1.0,\n",
       " 'A11': 1.0,\n",
       " 'A7': 2.0,\n",
       " 'A15': 2.0,\n",
       " 'P17': 1.0,\n",
       " 'P2': 2.0,\n",
       " 'A4': 2.0,\n",
       " 'B1': 1.0,\n",
       " 'P33': 1.0,\n",
       " 'P15': 1.0,\n",
       " 'B23': 2.0,\n",
       " 'C1': 2.0,\n",
       " 'B12': 2.0,\n",
       " 'B13': 1.0,\n",
       " 'B4': 1.0,\n",
       " 'P48': 2.0,\n",
       " 'A16': 2.0,\n",
       " 'B30': 1.0,\n",
       " 'P4': 1.0,\n",
       " 'P12': 2.0,\n",
       " 'A18': 2.0,\n",
       " 'P23': 2.0,\n",
       " 'B10': 1.0,\n",
       " 'A10': 2.0,\n",
       " 'A2': 2.0,\n",
       " 'B17': 2.0,\n",
       " 'C14': 2.0,\n",
       " 'B11': 2.0,\n",
       " 'A13': 2.0,\n",
       " 'A1': 2.0,\n",
       " 'A6': 2.0,\n",
       " 'C5': 1.0,\n",
       " 'A33': 2.0,\n",
       " 'C9': 1.0,\n",
       " 'B15': 2.0,\n",
       " 'C7': 1.0,\n",
       " 'C2': 1.0,\n",
       " 'B26': 1.0,\n",
       " 'B3': 1.0,\n",
       " 'B32': 2.0,\n",
       " 'C11': 2.0,\n",
       " 'P3': 1.0,\n",
       " 'C8': 1.0,\n",
       " 'B27': 1.0,\n",
       " 'P6': 1.0,\n",
       " 'B2': 1.0,\n",
       " 'C13': 1.0,\n",
       " 'B24': 1.0,\n",
       " 'B14': 1.0,\n",
       " 'B19': 2.0,\n",
       " 'C4': 1.0,\n",
       " 'B21': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_temp.duplicated().sum()\n",
    "df_temp.drop_duplicates(inplace= True)\n",
    "# df_temp.shape\n",
    "# df_test.shape\n",
    "\n",
    "# 2. Create the dictionary (key is the merge column, value is the column to transfer)\n",
    "# Only include the columns you need for the mapping\n",
    "map_dict = df_temp.set_index('page2_clothing_model_grouped')['model_target_encoded'].to_dict()\n",
    "map_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd31cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Use the .map() method to apply the new column to df_test\n",
    "# This is much cleaner and faster than using a lambda/apply for this operation.\n",
    "df_test['model_target_encoded'] = df_test['page2_clothing_model_grouped'].map(map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2668493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a5e71d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping + OHE\n",
    "# df_train['country'].unique()\n",
    "countries_grouped = [9,24,29,46]\n",
    "df_test['country_grouped'] = df_test['country'].apply(lambda x: 'RARE_GROUP' if x not in countries_grouped else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c5df93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. One-Hot Encoding for 'country_grouped' (The Simple, Safe Choice) ---\n",
    "# Use pandas get_dummies for simple OHE\n",
    "country_ohe = pd.get_dummies(df_test['country_grouped'], prefix='country', dtype=int)\n",
    "\n",
    "# Concatenate the new OHE columns to the main DataFrame\n",
    "df_test = pd.concat([df_test.drop('country_grouped', axis=1), country_ohe], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ebfc36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67509b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding (month_...) is complete and safe.\n",
      "One-Hot Encoding (page1_main_category_...) is complete and safe.\n",
      "One-Hot Encoding (colour_...) is complete and safe.\n",
      "One-Hot Encoding (location_...) is complete and safe.\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding\n",
    "ohe=['month','page1_main_category','colour','location']\n",
    "for i in ohe:\n",
    "    # Use pandas get_dummies for simple OHE\n",
    "    df_ohe = pd.get_dummies(df_test[i], prefix=i, dtype=int)\n",
    "\n",
    "    # Concatenate the new OHE columns to the main DataFrame\n",
    "    df_test = pd.concat([df_test.drop(i, axis=1), df_ohe], axis=1)\n",
    "\n",
    "    # Display the result (replace 'page2_clothing_model_grouped' with the encoded column)\n",
    "    # print(\"\\n--- Final Encoded Features Head ---\")\n",
    "    # print(data[['country_RARE_GROUP','model_target_encoded']].head())\n",
    "\n",
    "    \n",
    "    print(f\"One-Hot Encoding ({i}_...) is complete and safe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96d6509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the skewed features using the best transforming methods after analysing the skewness and kurtosis \n",
    "# df_test['price_transformed'], optimal_lambda = boxcox(df_test['price']) ------ this is wrong\n",
    "df_test['order_transformed'] = np.log(df_test['order'])\n",
    "# Winsorizing /Capping feature to upper bound\n",
    "df_test['order_capped'] = df_test['order_transformed'].apply(lambda i : 5.17 if i > 5.17 else i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c0087be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Apply the SAME optimal_lambda to the test data\n",
    "df_test['price_transformed'] = boxcox(df_test['price'], lmbda=optimal_lambda) #------------ this is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7610ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding\n",
    "df_test['model_photography'] = df_test['model_photography'].map({1:0, 2:1})\n",
    "# df_train['model_photography'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "089b7a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>day</th>\n",
       "      <th>order</th>\n",
       "      <th>country</th>\n",
       "      <th>session_id</th>\n",
       "      <th>page2_clothing_model</th>\n",
       "      <th>model_photography</th>\n",
       "      <th>price</th>\n",
       "      <th>price_2</th>\n",
       "      <th>page</th>\n",
       "      <th>...</th>\n",
       "      <th>colour_14</th>\n",
       "      <th>location_1</th>\n",
       "      <th>location_2</th>\n",
       "      <th>location_3</th>\n",
       "      <th>location_4</th>\n",
       "      <th>location_5</th>\n",
       "      <th>location_6</th>\n",
       "      <th>order_transformed</th>\n",
       "      <th>order_capped</th>\n",
       "      <th>price_transformed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31956</th>\n",
       "      <td>2008</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>1073</td>\n",
       "      <td>B14</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>6.044095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97097</th>\n",
       "      <td>2008</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>10903</td>\n",
       "      <td>A27</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>5.800530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90252</th>\n",
       "      <td>2008</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>11465</td>\n",
       "      <td>A14</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.976924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125226</th>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>14141</td>\n",
       "      <td>B28</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>5.080116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83215</th>\n",
       "      <td>2008</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>8210</td>\n",
       "      <td>C4</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>5.529871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  day  order  country  session_id page2_clothing_model  \\\n",
       "31956   2008    3      5       29        1073                  B14   \n",
       "97097   2008   24      2       29       10903                  A27   \n",
       "90252   2008   27      1       29       11465                  A14   \n",
       "125226  2008   12     11       29       14141                  B28   \n",
       "83215   2008    8      4        9        8210                   C4   \n",
       "\n",
       "        model_photography  price  price_2  page  ... colour_14  location_1  \\\n",
       "31956                   0     62        1     1  ...         0           0   \n",
       "97097                   0     55        1     2  ...         0           0   \n",
       "90252                   0     60        1     1  ...         0           0   \n",
       "125226                  0     38        2     2  ...         0           0   \n",
       "83215                   0     48        1     1  ...         0           0   \n",
       "\n",
       "        location_2  location_3  location_4  location_5  location_6  \\\n",
       "31956            0           0           0           1           0   \n",
       "97097            0           1           0           0           0   \n",
       "90252            0           0           0           1           0   \n",
       "125226           0           0           1           0           0   \n",
       "83215            1           0           0           0           0   \n",
       "\n",
       "        order_transformed  order_capped  price_transformed  \n",
       "31956            1.609438      1.609438           6.044095  \n",
       "97097            0.693147      0.693147           5.800530  \n",
       "90252            0.000000      0.000000           5.976924  \n",
       "125226           2.397895      2.397895           5.080116  \n",
       "83215            1.386294      1.386294           5.529871  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51a55fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # removal of unwanted features\n",
    "remove = ['year','price', 'day', 'order', 'country', 'session_id', 'page2_clothing_model',\n",
    "       'order_transformed', 'order_capped', 'page2_clothing_model_grouped']\n",
    "df_test.drop(columns=remove, inplace= True)\n",
    "df_train.drop(columns=remove, inplace= True)\n",
    "df_test.reset_index(drop=True, inplace= True)\n",
    "df_train.reset_index(drop=True, inplace= True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6d38622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_photography</th>\n",
       "      <th>price_2</th>\n",
       "      <th>page</th>\n",
       "      <th>price_transformed</th>\n",
       "      <th>model_target_encoded</th>\n",
       "      <th>country_9</th>\n",
       "      <th>country_24</th>\n",
       "      <th>country_29</th>\n",
       "      <th>country_46</th>\n",
       "      <th>country_RARE_GROUP</th>\n",
       "      <th>...</th>\n",
       "      <th>colour_11</th>\n",
       "      <th>colour_12</th>\n",
       "      <th>colour_13</th>\n",
       "      <th>colour_14</th>\n",
       "      <th>location_1</th>\n",
       "      <th>location_2</th>\n",
       "      <th>location_3</th>\n",
       "      <th>location_4</th>\n",
       "      <th>location_5</th>\n",
       "      <th>location_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5.080116</td>\n",
       "      <td>1.547368</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5.529871</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.632712</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.355335</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4.519170</td>\n",
       "      <td>1.545677</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105898</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4.817257</td>\n",
       "      <td>1.545677</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105899</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5.080116</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105900</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.529871</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105901</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.817257</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105902</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5.315813</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105903 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        model_photography  price_2  page  price_transformed  \\\n",
       "0                       0        2     3           5.080116   \n",
       "1                       0        2     1           5.529871   \n",
       "2                       0        1     1           6.632712   \n",
       "3                       0        1     1           6.355335   \n",
       "4                       1        2     4           4.519170   \n",
       "...                   ...      ...   ...                ...   \n",
       "105898                  0        2     2           4.817257   \n",
       "105899                  1        2     1           5.080116   \n",
       "105900                  0        1     1           5.529871   \n",
       "105901                  0        2     1           4.817257   \n",
       "105902                  0        2     1           5.315813   \n",
       "\n",
       "        model_target_encoded  country_9  country_24  country_29  country_46  \\\n",
       "0                   1.547368          0           0           1           0   \n",
       "1                   2.000000          0           0           1           0   \n",
       "2                   1.000000          1           0           0           0   \n",
       "3                   1.000000          0           0           1           0   \n",
       "4                   1.545677          0           0           1           0   \n",
       "...                      ...        ...         ...         ...         ...   \n",
       "105898              1.545677          0           0           1           0   \n",
       "105899              2.000000          0           0           1           0   \n",
       "105900              1.000000          0           0           0           0   \n",
       "105901              2.000000          0           0           1           0   \n",
       "105902              2.000000          0           0           1           0   \n",
       "\n",
       "        country_RARE_GROUP  ...  colour_11  colour_12  colour_13  colour_14  \\\n",
       "0                        0  ...          0          0          0          1   \n",
       "1                        0  ...          0          0          0          0   \n",
       "2                        0  ...          0          0          0          0   \n",
       "3                        0  ...          0          0          0          0   \n",
       "4                        0  ...          0          0          0          0   \n",
       "...                    ...  ...        ...        ...        ...        ...   \n",
       "105898                   0  ...          0          0          0          0   \n",
       "105899                   0  ...          0          0          0          0   \n",
       "105900                   1  ...          0          0          0          0   \n",
       "105901                   0  ...          0          0          0          0   \n",
       "105902                   0  ...          0          0          0          0   \n",
       "\n",
       "        location_1  location_2  location_3  location_4  location_5  location_6  \n",
       "0                0           0           0           0           0           1  \n",
       "1                0           0           1           0           0           0  \n",
       "2                0           0           1           0           0           0  \n",
       "3                1           0           0           0           0           0  \n",
       "4                0           0           1           0           0           0  \n",
       "...            ...         ...         ...         ...         ...         ...  \n",
       "105898           1           0           0           0           0           0  \n",
       "105899           0           0           0           0           0           1  \n",
       "105900           1           0           0           0           0           0  \n",
       "105901           0           0           0           0           0           1  \n",
       "105902           0           0           1           0           0           0  \n",
       "\n",
       "[105903 rows x 39 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd44426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant features\n",
    "# ['price', --- not req - done\n",
    "#  'price_transformed', --- cont\n",
    "#  'month',5 --- OHE\n",
    "#  'country',46 --- grouped and OHE  - done\n",
    "#  'session_id', --- unique (not req) - to be removed-----------------------------------------\n",
    "#  'page2_clothing_model',216 --- Target encode - done\n",
    "#  'page1_main_category',4 --- OHE\n",
    "#  'colour',14 --- OHE to be grouped and OHE\n",
    "#  'location',6 --- OHE \n",
    "#  'model_photography',2 --- OHE - label(binary) with 0/1\n",
    "#  'page', --- label(ordinal) - already done\n",
    "#  'model_encoded'] --- already done a frequecny ----to be removed-------------------------------\n",
    "# ohe=['month','page1_main_category','colour','location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5af742f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns == df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0781a448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['model_photography', 'price_2', 'page', 'price_transformed',\n",
       "       'model_target_encoded', 'country_9', 'country_24', 'country_29',\n",
       "       'country_46', 'country_RARE_GROUP', 'month_4', 'month_5', 'month_6',\n",
       "       'month_7', 'month_8', 'page1_main_category_1', 'page1_main_category_2',\n",
       "       'page1_main_category_3', 'page1_main_category_4', 'colour_1',\n",
       "       'colour_2', 'colour_3', 'colour_4', 'colour_5', 'colour_6', 'colour_7',\n",
       "       'colour_8', 'colour_9', 'colour_10', 'colour_11', 'colour_12',\n",
       "       'colour_13', 'colour_14', 'location_1', 'location_2', 'location_3',\n",
       "       'location_4', 'location_5', 'location_6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05a6291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.columns\n",
    "adjust_col =['model_photography', 'price_2', 'page','price_transformed','model_target_encoded',\n",
    "       'country_9', 'country_24', 'country_29', 'country_46',\n",
    "       'country_RARE_GROUP', 'month_4', 'month_5', 'month_6', 'month_7',\n",
    "       'month_8', 'page1_main_category_1', 'page1_main_category_2',\n",
    "       'page1_main_category_3', 'page1_main_category_4', 'colour_1',\n",
    "       'colour_2', 'colour_3', 'colour_4', 'colour_5', 'colour_6', 'colour_7',\n",
    "       'colour_8', 'colour_9', 'colour_10', 'colour_11', 'colour_12',\n",
    "       'colour_13', 'colour_14', 'location_1', 'location_2', 'location_3',\n",
    "       'location_4', 'location_5', 'location_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8d5130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[adjust_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Reindex the DataFrame with the new order\n",
    "# df = df[other_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c573f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_2720\\1661807899.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test.drop(columns='price',inplace= True)\n"
     ]
    }
   ],
   "source": [
    "# df_train.drop(columns='price',inplace= True)\n",
    "# df_test.drop(columns='price',inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee03b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you want to swap 'C' and 'B'\n",
    "cols = list(df_test.columns) # ['A', 'C', 'B'] from the previous step\n",
    "\n",
    "# Find the indices of the columns you want to swap\n",
    "index_model_target_encoded = cols.index('model_target_encoded') # 1\n",
    "index_price_transformed = cols.index('price_transformed') # 2\n",
    "\n",
    "# Perform the swap in the list\n",
    "cols[index_model_target_encoded], cols[index_price_transformed] = cols[index_price_transformed], cols[index_model_target_encoded]\n",
    "\n",
    "# Apply the new order\n",
    "df_test = df_test[cols]\n",
    "\n",
    "print(df)\n",
    "# df now looks like: A B C (Swapped back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ee9f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"df_train_preprocessed.csv\")\n",
    "df_test.to_csv(\"df_test_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a863d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================================\n",
    "#======================================================================\n",
    "#======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "435dc75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prep - Classification - Randomforest\n",
    "# Target - price_2\n",
    "# import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1479134",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train.drop(columns=['price_2'])\n",
    "y_train = df_train['price_2']\n",
    "x_test = df_test.drop(columns=['price_2'])\n",
    "y_test = df_test['price_2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2c07b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Random Forest Classifier\n",
    "# n_estimators: Number of trees in the forest (try 100, 200, or more)\n",
    "# random_state: For reproducibility\n",
    "# n_jobs: Use -1 to utilize all available processors\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    # criterion='gini',\n",
    "    # max_depth=None, # Let trees grow fully (default)\n",
    "    # min_samples_split=2,\n",
    "    random_state=42\n",
    "    # n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74692580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting model training...\")\n",
    "rf_model.fit(x_train, y_train)\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9cb2c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class labels for the test set\n",
    "y_pred = rf_model.predict(x_test)\n",
    "\n",
    "# You can also get the predicted probabilities (useful for ROC curves, etc.)\n",
    "y_pred_proba = rf_model.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "344955f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Score: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00     13556\n",
      "           2       1.00      1.00      1.00     12920\n",
      "\n",
      "    accuracy                           1.00     26476\n",
      "   macro avg       1.00      1.00      1.00     26476\n",
      "weighted avg       1.00      1.00      1.00     26476\n",
      "\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "price_transformed        0.477240\n",
      "model_target_encoded     0.278071\n",
      "page1_main_category_4    0.032349\n",
      "page                     0.031230\n",
      "page1_main_category_1    0.027273\n",
      "page1_main_category_2    0.014745\n",
      "model_photography        0.011170\n",
      "colour_1                 0.010611\n",
      "page1_main_category_3    0.009363\n",
      "location_1               0.009342\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1. Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy Score: {accuracy:.4f}\")\n",
    "\n",
    "# 2. Detailed Classification Report (Precision, Recall, F1-Score)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 3. Feature Importance (A key benefit of Random Forest)\n",
    "feature_importances = pd.Series(\n",
    "    rf_model.feature_importances_,\n",
    "    index=x_train.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Feature Importances:\")\n",
    "print(feature_importances.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================================================================================================================\n",
    "#======================================================================================================================================================\n",
    "#======================================================================================================================================================\n",
    "#======================================================================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8d94af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33095, 14)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting a single entry\n",
    "data_test = pd.read_csv(\"D:\\GIT HUB\\GUVI Mini Proj 4\\\\test_data - test_data.csv\")\n",
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1894223a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>order</th>\n",
       "      <th>country</th>\n",
       "      <th>session_id</th>\n",
       "      <th>page1_main_category</th>\n",
       "      <th>page2_clothing_model</th>\n",
       "      <th>colour</th>\n",
       "      <th>location</th>\n",
       "      <th>model_photography</th>\n",
       "      <th>price</th>\n",
       "      <th>price_2</th>\n",
       "      <th>page</th>\n",
       "      <th>page2_clothing_model_grouped</th>\n",
       "      <th>model_target_encoded</th>\n",
       "      <th>price_transformed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>5279</td>\n",
       "      <td>4</td>\n",
       "      <td>P48</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>P48</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.817257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>10059</td>\n",
       "      <td>1</td>\n",
       "      <td>A15</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A15</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.817257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "      <td>2919</td>\n",
       "      <td>4</td>\n",
       "      <td>P23</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>P23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.519170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>6304</td>\n",
       "      <td>2</td>\n",
       "      <td>B24</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>B24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.872616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>11266</td>\n",
       "      <td>1</td>\n",
       "      <td>A2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.315813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  order  country  session_id  page1_main_category  \\\n",
       "0  2008      4   22      4       29        5279                    4   \n",
       "1  2008      5   19      1       29       10059                    1   \n",
       "2  2008      4   11     10       29        2919                    4   \n",
       "3  2008      4   28      3       27        6304                    2   \n",
       "4  2008      5   26      1       29       11266                    1   \n",
       "\n",
       "  page2_clothing_model  colour  location  model_photography  price  price_2  \\\n",
       "0                  P48       9         4                  2     33        2   \n",
       "1                  A15      14         5                  2     33        2   \n",
       "2                  P23       6         2                  2     28        2   \n",
       "3                  B24      11         2                  1     57        1   \n",
       "4                   A2       3         1                  1     43        2   \n",
       "\n",
       "   page page2_clothing_model_grouped  model_target_encoded  price_transformed  \n",
       "0     3                          P48                   2.0           4.817257  \n",
       "1     1                          A15                   2.0           4.817257  \n",
       "2     2                          P23                   2.0           4.519170  \n",
       "3     2                          B24                   1.0           5.872616  \n",
       "4     1                           A2                   2.0           5.315813  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0439ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp = x_test.iloc[0].to_dict()\n",
    "# samp_test = data_test.iloc[0].to_dict()\n",
    "samp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e94f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding and transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "973e9469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target encoding of model in data_test\n",
    "# page2_groups = ['RARE_GROUP', 'B9', 'A9', 'A3', 'P1', 'C56', 'A31', 'C17', 'C49',\n",
    "#        'B31', 'A12', 'C50', 'C12', 'B16', 'A8', 'A5', 'A14', 'C40', 'A17',\n",
    "#        'P16', 'A21', 'A11', 'A7', 'A15', 'P17', 'P2', 'A4', 'B1', 'P33',\n",
    "#        'P15', 'B23', 'C1', 'B12', 'B13', 'B4', 'P48', 'A16', 'B30', 'P4',\n",
    "#        'P12', 'A18', 'P23', 'B10', 'A10', 'A2', 'B17', 'C14', 'B11',\n",
    "#        'A13', 'A1', 'A6', 'C5', 'A33', 'C9', 'B15', 'C7', 'C2', 'B26',\n",
    "#        'B3', 'B32', 'C11', 'P3', 'C8', 'B27', 'P6', 'B2', 'C13', 'B24',\n",
    "#        'B14', 'B19', 'C4', 'B21']\n",
    "data_test['page2_clothing_model_grouped'] = data_test['page2_clothing_model'].apply(lambda x: 'RARE_GROUP' if x not in page2_groups else x)\n",
    "# df_test['page2_clothing_model_grouped'].unique()\n",
    "# 3. Use the .map() method to apply the new column to df_test\n",
    "# This is much cleaner and faster than using a lambda/apply for this operation.\n",
    "data_test['model_target_encoded'] = data_test['page2_clothing_model_grouped'].map(map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae96472e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_test['price'] transformed using the lambda from X_train.\n"
     ]
    }
   ],
   "source": [
    "# 2. Apply the SAME optimal_lambda to the test data\n",
    "data_test['price_transformed'] = boxcox(data_test['price'], lmbda=optimal_lambda)\n",
    "\n",
    "print(\"data_test['price'] transformed using the lambda from X_train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fd521de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping + OHE\n",
    "# df_train['country'].unique()\n",
    "# countries_grouped = [9,24,29,46]\n",
    "data_test['country_grouped'] = data_test['country'].apply(lambda x: 'RARE_GROUP' if x not in countries_grouped else x)\n",
    "# --- 2. One-Hot Encoding for 'country_grouped' (The Simple, Safe Choice) ---\n",
    "# Use pandas get_dummies for simple OHE\n",
    "country_ohe = pd.get_dummies(data_test['country_grouped'], prefix='country', dtype=int)\n",
    "\n",
    "# Concatenate the new OHE columns to the main DataFrame\n",
    "data_test = pd.concat([data_test.drop('country_grouped', axis=1), country_ohe], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d006bff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding (month_...) is complete and safe.\n",
      "One-Hot Encoding (page1_main_category_...) is complete and safe.\n",
      "One-Hot Encoding (colour_...) is complete and safe.\n",
      "One-Hot Encoding (location_...) is complete and safe.\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding\n",
    "ohe=['month','page1_main_category','colour','location']\n",
    "for i in ohe:\n",
    "    # Use pandas get_dummies for simple OHE\n",
    "    df_ohe = pd.get_dummies(data_test[i], prefix=i, dtype=int)\n",
    "\n",
    "    # Concatenate the new OHE columns to the main DataFrame\n",
    "    data_test = pd.concat([data_test.drop(i, axis=1), df_ohe], axis=1)\n",
    "\n",
    "    # Display the result (replace 'page2_clothing_model_grouped' with the encoded column)\n",
    "    # print(\"\\n--- Final Encoded Features Head ---\")\n",
    "    # print(data[['country_RARE_GROUP','model_target_encoded']].head())\n",
    "\n",
    "    \n",
    "    print(f\"One-Hot Encoding ({i}_...) is complete and safe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bba9a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding\n",
    "data_test['model_photography'] = data_test['model_photography'].map({1:0, 2:1})\n",
    "# df_train['model_photography'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a972b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ad08453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # removal of unwanted features\n",
    "remove = ['year','price', 'day', 'order', 'country', 'session_id', 'page2_clothing_model','page2_clothing_model_grouped']\n",
    "data_test.drop(columns=remove, inplace= True)\n",
    "data_test.reset_index(drop=True, inplace= True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4522822c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns == data_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fcce888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test[adjust_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b26382d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.to_csv(\"data_test(not_used_in moded_training_preprocessed).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5fcac5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test1 = data_test.drop(columns='price_2')\n",
    "y_test1 = data_test['price_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4880a7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f9c969cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class labels for the test set\n",
    "y_pred = rf_model.predict(x_test1)\n",
    "\n",
    "# You can also get the predicted probabilities (useful for ROC curves, etc.)\n",
    "y_pred_proba = rf_model.predict_proba(x_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "48c7d3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Score: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00     16981\n",
      "           2       1.00      1.00      1.00     16114\n",
      "\n",
      "    accuracy                           1.00     33095\n",
      "   macro avg       1.00      1.00      1.00     33095\n",
      "weighted avg       1.00      1.00      1.00     33095\n",
      "\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "price_transformed        0.477240\n",
      "model_target_encoded     0.278071\n",
      "page1_main_category_4    0.032349\n",
      "page                     0.031230\n",
      "page1_main_category_1    0.027273\n",
      "page1_main_category_2    0.014745\n",
      "model_photography        0.011170\n",
      "colour_1                 0.010611\n",
      "page1_main_category_3    0.009363\n",
      "location_1               0.009342\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1. Accuracy\n",
    "accuracy = accuracy_score(y_test1, y_pred)\n",
    "print(f\"\\nAccuracy Score: {accuracy:.4f}\")\n",
    "\n",
    "# 2. Detailed Classification Report (Precision, Recall, F1-Score)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test1, y_pred))\n",
    "\n",
    "# 3. Feature Importance (A key benefit of Random Forest)\n",
    "feature_importances = pd.Series(\n",
    "    rf_model.feature_importances_,\n",
    "    index=x_train.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Feature Importances:\")\n",
    "print(feature_importances.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb858184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63352e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {'model_photography': 1.0,\n",
    " 'page': 3.0,\n",
    " 'price_transformed': 4.817256942779583,\n",
    " 'model_target_encoded': 2.0,\n",
    " 'country_9': 0.0,\n",
    " 'country_24': 0.0,\n",
    " 'country_29': 1.0,\n",
    " 'country_46': 0.0,\n",
    " 'country_RARE_GROUP': 0.0,\n",
    " 'month_4': 1.0,\n",
    " 'month_5': 0.0,\n",
    " 'month_6': 0.0,\n",
    " 'month_7': 0.0,\n",
    " 'month_8': 0.0,\n",
    " 'page1_main_category_1': 0.0,\n",
    " 'page1_main_category_2': 0.0,\n",
    " 'page1_main_category_3': 0.0,\n",
    " 'page1_main_category_4': 1.0,\n",
    " 'colour_1': 0.0,\n",
    " 'colour_2': 0.0,\n",
    " 'colour_3': 0.0,\n",
    " 'colour_4': 0.0,\n",
    " 'colour_5': 0.0,\n",
    " 'colour_6': 0.0,\n",
    " 'colour_7': 0.0,\n",
    " 'colour_8': 0.0,\n",
    " 'colour_9': 1.0,\n",
    " 'colour_10': 0.0,\n",
    " 'colour_11': 0.0,\n",
    " 'colour_12': 0.0,\n",
    " 'colour_13': 0.0,\n",
    " 'colour_14': 0.0,\n",
    " 'location_1': 0.0,\n",
    " 'location_2': 0.0,\n",
    " 'location_3': 0.0,\n",
    " 'location_4': 1.0,\n",
    " 'location_5': 0.0,\n",
    " 'location_6': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbeae8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int64' object has no attribute 'to_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m samp_test = \u001b[43my_test1\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dict\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.int64' object has no attribute 'to_dict'"
     ]
    }
   ],
   "source": [
    "samp_test = data_test.iloc[3].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1e904b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-row DataFrame created successfully:\n",
      "   model_photography  price_2  page  price_transformed  model_target_encoded  \\\n",
      "0                0.0      1.0   2.0           5.872616                   1.0   \n",
      "\n",
      "   country_9  country_24  country_29  country_46  country_RARE_GROUP  ...  \\\n",
      "0        0.0         0.0         0.0         0.0                 1.0  ...   \n",
      "\n",
      "   colour_11  colour_12  colour_13  colour_14  location_1  location_2  \\\n",
      "0        1.0        0.0        0.0        0.0         0.0         1.0   \n",
      "\n",
      "   location_3  location_4  location_5  location_6  \n",
      "0         0.0         0.0         0.0         0.0  \n",
      "\n",
      "[1 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "# The fix: Wrap all values in lists using a dictionary comprehension\n",
    "input_data_fixed = {k: [v] for k, v in samp_test.items()}\n",
    "\n",
    "# Create the DataFrame\n",
    "single_entry_df = pd.DataFrame(input_data_fixed)\n",
    "\n",
    "print(\"Single-row DataFrame created successfully:\")\n",
    "print(single_entry_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9d3d6949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class Label: 1\n"
     ]
    }
   ],
   "source": [
    "# single_entry_df = pd.DataFrame(input_data)\n",
    "\n",
    "# Predict the class label for the single entry\n",
    "single_entry_df.drop(columns='price_2', inplace = True)\n",
    "prediction_label = rf_model.predict(single_entry_df)\n",
    "\n",
    "print(f\"Predicted Class Label: {prediction_label[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clickstream_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
