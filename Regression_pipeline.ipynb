{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session plan\n",
    "# Create a pipeline for the regression model \n",
    "#   it should pre-process and also train the model \n",
    "#   use it to predict a outcome\n",
    "# Use the created pipeline to train the model and log it with mlflow.\n",
    "# Use the logged model to predict a an outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6af3c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from scipy.stats import skew\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, chi2_contingency, f_oneway\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "import numpy as np\n",
    "from scipy.stats import boxcox, skew\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a167d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:\\\\GIT HUB\\\\GUVI Mini Proj 4\\\\train_data - train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0021f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data.drop(columns=['price'])\n",
    "df_test = data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eec6bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df_train,df_test, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326f034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Building Pipeline for preprocessing categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d383b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be done: Grouping, Target encoding, OHE,Binary,label, cyclic encoding and Scaling on target encoded\n",
    "# done:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5894fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cyclic good\n",
    "# cyclic encoding function for month and day\n",
    "import numpy as np\n",
    "\n",
    "def cyclic_encode(df, col, max_val):\n",
    "    \"\"\"\n",
    "    Applies sine and cosine transformation to a single column for cyclic encoding.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        col (str): The column to transform (e.g., 'hour').\n",
    "        max_val (int): The maximum possible value of the feature (e.g., 24 for hour).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the two new encoded columns.\n",
    "    \"\"\"\n",
    "    df[col + '_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
    "    df[col + '_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
    "    # Drop the original column as it is now represented by the sine and cosine features\n",
    "    df = df.drop(col, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e758d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Defining the pre-processing steps ---\n",
    "\n",
    "# Define a function to apply the encoding to the dataframe.\n",
    "# We wrap the specific logic within a lambda for the ColumnTransformer\n",
    "# to only pass the required column and context.\n",
    "def apply_cyclic_encoding(X, col, max_val):\n",
    "    # X is a numpy array when passed through ColumnTransformer, so convert it back to DataFrame\n",
    "    # Note: ColumnTransformer passes the *entire* column/array slice.\n",
    "    X_df = pd.DataFrame(X, columns=[col])\n",
    "    return cyclic_encode(X_df, col, max_val)\n",
    "\n",
    "# Create the transformers\n",
    "cyclic_hour_transformer = FunctionTransformer(\n",
    "    func=lambda X: apply_cyclic_encoding(X, 'month', 12),\n",
    "    validate=False # Set to False to handle DataFrame/numpy array mix\n",
    ")\n",
    "\n",
    "cyclic_day_transformer = FunctionTransformer(\n",
    "    func=lambda X: apply_cyclic_encoding(X, 'day', 31),\n",
    "    validate=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1145e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping, Target encoding, OHE,Binary,label and Scaling\n",
    "# done:cyclic encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d4bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old code\n",
    "# Groupping \n",
    "# import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class RareCategoryGrouper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Groups categories in a specified column that fall below a minimum\n",
    "    percentage threshold into a single 'RARE_GROUP'.\n",
    "    \"\"\"\n",
    "    def __init__(self, column_name, min_percentage=0.01, rare_label='RARE_GROUP'):\n",
    "        self.column_name = column_name\n",
    "        self.min_percentage = min_percentage\n",
    "        self.rare_label = rare_label\n",
    "        self.rare_categories_ = None # Stores the categories learned during fit\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Calculates and stores the list of categories that are below the\n",
    "        min_percentage threshold in the training data (X).\n",
    "        \"\"\"\n",
    "        # 1. Calculate the normalized frequency counts\n",
    "        value_counts = X[self.column_name].value_counts(normalize=True)\n",
    "\n",
    "        # 2. Identify and store the categories to be grouped\n",
    "        self.rare_categories_ = value_counts[value_counts < self.min_percentage].index.tolist()\n",
    "\n",
    "        if not self.rare_categories_:\n",
    "            print(f\"FIT: No categories in '{self.column_name}' below the {self.min_percentage*100:.2f}% threshold.\")\n",
    "        else:\n",
    "             print(f\"FIT: Identified {len(self.rare_categories_)} rare categories in '{self.column_name}'.\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Applies the grouping transformation using the stored rare categories.\n",
    "        \"\"\"\n",
    "        # Ensure we operate on a copy to avoid side effects on the input DataFrame\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # If no rare categories were found during fit, return the DataFrame unchanged\n",
    "        if not self.rare_categories_:\n",
    "            return X_copy\n",
    "        \n",
    "        # 3. Create the new column name\n",
    "        new_column_name = f'{self.column_name}_grouped'\n",
    "        X_copy[new_column_name] = X_copy[self.column_name]\n",
    "\n",
    "        # 4. Apply the grouping using .loc\n",
    "        X_copy.loc[X_copy[self.column_name].isin(self.rare_categories_), new_column_name] = self.rare_label\n",
    "        \n",
    "        # IMPORTANT: Drop the original column to maintain a consistent column count in the pipeline\n",
    "        X_copy = X_copy.drop(columns=[self.column_name])\n",
    "        \n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a60621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old code\n",
    "# --- Define the Grouping Steps ---\n",
    "# Group 'country' with a 1% threshold\n",
    "country_grouper = RareCategoryGrouper(column_name='country', min_percentage=0.01)\n",
    "\n",
    "# Group 'page2_clothing_model' with a 0.5% threshold\n",
    "model_grouper = RareCategoryGrouper(column_name='page2_clothing_model', min_percentage=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0a7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Target Encoder with CV and Box-Cox ---\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from category_encoders import TargetEncoder\n",
    "from scipy.stats import boxcox # For Box-Cox transformation\n",
    "class TargetEncoderCV(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Performs K-Fold Cross-Validated Target Encoding during fit, followed by Box-Cox scaling.\n",
    "    Accepts the column data from the previous pipeline step.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits=5, random_state=42):\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.full_encoder_ = None \n",
    "        self.lambda_ = None       \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # X is the data slice (now the grouped column array from RareCategoryGrouper)\n",
    "        X_series = pd.Series(X.flatten(), index=y.index)\n",
    "        \n",
    "        # 1. Cross-Validated Encoding\n",
    "        kf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "        X_encoded_train = pd.Series(np.nan, index=y.index)\n",
    "\n",
    "        for train_index, val_index in kf.split(X_series.to_frame(), y):\n",
    "            encoder = TargetEncoder()\n",
    "            # Fit only on the K-1 folds (uses Series access)\n",
    "            encoder.fit(X_series.iloc[train_index], y.iloc[train_index])\n",
    "            # Transform the validation fold\n",
    "            X_encoded_train.loc[val_index] = encoder.transform(X_series.iloc[val_index]).flatten()\n",
    "            \n",
    "        # 2. Box-Cox Transformation Parameter Learning\n",
    "        X_encoded_train_2d = X_encoded_train.values.reshape(-1, 1)\n",
    "        X_boxcox, self.lambda_ = boxcox(X_encoded_train_2d + 1e-6) \n",
    "\n",
    "        # 3. Store the full encoder for transformation of *new* data\n",
    "        self.full_encoder_ = TargetEncoder()\n",
    "        self.full_encoder_.fit(X_series, y)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # X is the data slice from the previous step\n",
    "        X_series = pd.Series(X.flatten(), index=pd.Index(range(len(X))))\n",
    "        \n",
    "        # 1. Apply Target Encoding using the FULL ENCODER learned during fit\n",
    "        X_encoded = self.full_encoder_.transform(X_series)\n",
    "        \n",
    "        # 2. Apply Box-Cox Transformation using the stored lambda\n",
    "        X_transformed = boxcox(X_encoded.values.reshape(-1, 1) + 1e-6, lmbda=self.lambda_)\n",
    "\n",
    "        return X_transformed.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping new\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# --- 1. Rare Category Grouper (Returns only the new column array) ---\n",
    "class RareCategoryGrouper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Groups categories that fall below a minimum percentage threshold into a single 'RARE_GROUP'.\n",
    "    This version is designed for use in a scikit-learn Pipeline and returns only the transformed column data.\n",
    "    \"\"\"\n",
    "    def __init__(self, rare_label='RARE_GROUP', min_percentage=0.01):\n",
    "        # We don't need 'column_name' here because the ColumnTransformer will select the column for us.\n",
    "        self.rare_label = rare_label\n",
    "        self.min_percentage = min_percentage\n",
    "        self.rare_categories_ = None # Stores the categories to be grouped\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # X is the slice of data (e.g., the 'page2_clothing_model_grouped' column data)\n",
    "        # Convert array slice back to Series for value_counts\n",
    "        X_series = pd.Series(X.flatten())\n",
    "\n",
    "        value_counts = X_series.value_counts(normalize=True)\n",
    "        self.rare_categories_ = value_counts[value_counts < self.min_percentage].index.tolist()\n",
    "        \n",
    "        # We don't print inside fit/transform methods in production code, but keeping for explanation:\n",
    "        # print(f\"FIT: Identified {len(self.rare_categories_)} rare categories.\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_series = pd.Series(X.flatten(), index=pd.Index(range(len(X))))\n",
    "        \n",
    "        # Create a copy to modify\n",
    "        X_transformed = X_series.copy()\n",
    "        \n",
    "        if self.rare_categories_:\n",
    "            # Apply the grouping based on the categories learned during fit\n",
    "            X_transformed[X_series.isin(self.rare_categories_)] = self.rare_label\n",
    "        \n",
    "        # Return the transformed column data as a 2D array, which is required by scikit-learn\n",
    "        return X_transformed.values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1430127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define the Mini-Pipeline for Sequential Steps ---\n",
    "# This pipeline performs two actions on the same column sequentially: Grouping then Encoding/Scaling\n",
    "target_encoding_and_scaling_pipe = Pipeline(steps=[\n",
    "    # Step 1: Group rare categories in the selected column\n",
    "    ('group_rare', RareCategoryGrouper(min_percentage=0.005)), \n",
    "    # Step 2: Apply K-Fold Target Encoding and Box-Cox Scaling to the grouped result\n",
    "    ('target_boxcox', TargetEncoderCV(n_splits=5))\n",
    "])\n",
    "\n",
    "country_grouper_OHE_pipe = Pipeline(steps = [('group_rare', RareCategoryGrouper(min_percentage= 0.01)),\n",
    "                                             ('ohe_country', ohe_encoder())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e659a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the grouping+target encoder+boxcox pipeline \n",
    "# complete the grouping + OHE\n",
    "# Complete the OHE\n",
    "# compile the 1st 2 pipelines with other steps in the columntransformer\n",
    "# compile the final pre-processer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f8f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoding, OHE,label and Scaling\n",
    "# done:cyclic encoding , Grouping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e46fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding  - good\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class DictionaryMapper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Applies a fixed dictionary mapping (Label Encoding) to a specified column.\n",
    "    \"\"\"\n",
    "    def __init__(self, column_name, mapping_dict):\n",
    "        self.column_name = column_name\n",
    "        self.mapping_dict = mapping_dict\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No fitting is required as the mapping is pre-defined\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Apply the mapping and replace the original column\n",
    "        X_copy[self.column_name] = X_copy[self.column_name].map(self.mapping_dict)\n",
    "        \n",
    "        # NOTE: .map() will produce NaN for values not in the dictionary. \n",
    "        # If your data might contain other values, you may need a fillna() or error check here.\n",
    "        \n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3cc4a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mappers using your specified dictionary\n",
    "model_photo_mapper = DictionaryMapper(\n",
    "    column_name='model_photography', \n",
    "    mapping_dict={1: 0, 2: 1}\n",
    ")\n",
    "\n",
    "price_mapper = DictionaryMapper(\n",
    "    column_name='price_2', \n",
    "    mapping_dict={1: 0, 2: 1}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoding, OHE and Scaling\n",
    "# done:cyclic encoding , Grouping, label encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54bc48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#good - grouping\n",
    "# --- Custom Transformer for Rare Category Grouping (FIXED) ---\n",
    "class RareCategoryGrouper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer to group categories whose frequency is below a given threshold\n",
    "    into a single 'RARE_GROUP' category.\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold=0.01, rare_group_name='RARE_GROUP'):\n",
    "        # The threshold (e.g., 0.01 for 1%) below which categories are grouped\n",
    "        self.threshold = threshold\n",
    "        # The name assigned to the grouped rare categories\n",
    "        self.rare_group_name = rare_group_name\n",
    "        # Internal attribute to store the list of frequent categories (learned during fit)\n",
    "        self.frequent_categories_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Determine the Series to work with (handles DataFrame or Series input)\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # If it's a DataFrame, extract the first (and only) column\n",
    "            X_series = X.iloc[:, 0]\n",
    "        else:\n",
    "            X_series = X\n",
    "        \n",
    "        # Calculate the frequency of each category\n",
    "        category_counts = X_series.value_counts(normalize=True)\n",
    "        \n",
    "        # Identify categories that meet or exceed the threshold\n",
    "        self.frequent_categories_ = category_counts[category_counts >= self.threshold].index.tolist()\n",
    "        \n",
    "        # Return self to adhere to scikit-learn API\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # --- FIX APPLIED HERE ---\n",
    "        # 1. Determine the column content to transform (Series)\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # Extract the single column content as a Series\n",
    "            series_to_process = X.iloc[:, 0]\n",
    "            name_to_keep = X.columns[0]\n",
    "        else: # Must be a Series\n",
    "            series_to_process = X\n",
    "            name_to_keep = X.name\n",
    "            \n",
    "        # Perform the actual grouping transformation on the Series data (using numpy.where)\n",
    "        grouped_values = np.where(\n",
    "            series_to_process.isin(self.frequent_categories_), \n",
    "            series_to_process, \n",
    "            self.rare_group_name\n",
    "        )\n",
    "\n",
    "        # 2. Re-wrap the results as a Series, maintaining index and original column name\n",
    "        X_transformed_series = pd.Series(\n",
    "            grouped_values, \n",
    "            index=X.index, \n",
    "            name=name_to_keep\n",
    "        )\n",
    "        \n",
    "        # 3. Always return a DataFrame (2D array-like structure) by calling to_frame() on the Series\n",
    "        return X_transformed_series.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ae8de2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Grouping_OHE_pipeline = Pipeline(steps=[\n",
    "    (\"Grouping\",RareCategoryGrouper(threshold=0.01)),\n",
    "    (\"OHE\",OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cdbe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoding, OHE and Scaling - only one column left(clothing model)\n",
    "# done:cyclic encoding , Grouping, label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Build the ColumnTransformer ---\n",
    "# Categorical \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Apply cyclic encoding to 'hour' and 'day_of_week'\n",
    "        ('cyclic_hour', cyclic_hour_transformer, ['month']),\n",
    "        ('cyclic_day', cyclic_day_transformer, ['day']),\n",
    "\n",
    "        # Label Encoding (NO SCALING APPLIED)\n",
    "        # Use the mappers directly here. The output is 0/1, which is final.\n",
    "        ('map_model_photo', model_photo_mapper, ['model_photography']),\n",
    "        ('map_price_2', price_mapper, ['price_2']),\n",
    "\n",
    "        # ONE-HOT ENCODING STEP (New) \n",
    "        ('simple OHE', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['page1_main_category','colour','location']),\n",
    "        \n",
    "        #Grouping pipe\n",
    "        (\"Grouping+OHE\",Grouping_OHE_pipeline,['country'])\n",
    "\n",
    "\n",
    "        # ------------ old code-----------\n",
    "        # 2. Rare Category Grouping (Categorical Features)\n",
    "        # ('group_country', country_grouper, ['country']),\n",
    "        # ('group_model', model_grouper, ['page2_clothing_model']),\n",
    "\n",
    "        \n",
    "        # ONE-HOT ENCODING STEP (New)\n",
    "        # ('country_grouper_OHE_pipeline', country_grouper, ['country'])\n",
    "\n",
    "\n",
    "        # Group then Target Encoding/Box-Cox (for model)\n",
    "        # ('group_target_boxcox', target_encoding_and_scaling_pipe, ['page2_clothing_model']),\n",
    "        \n",
    "    ],\n",
    "    remainder='passthrough' # Keep any other columns if they exist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aaff20d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed = preprocessor.fit_transform(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5615057d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4999999999999997, -0.8660254037844388, 0.72479278722912,\n",
       "        0.6889669190756866, 0, 1, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "        0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n",
       "        0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 2008, 2, 17592, 'C53', 3]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_processed[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4aa6d8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.724793</td>\n",
       "      <td>0.688967</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>2</td>\n",
       "      <td>17592</td>\n",
       "      <td>C53</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.97953</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>6</td>\n",
       "      <td>12203</td>\n",
       "      <td>B9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.101168</td>\n",
       "      <td>-0.994869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>5</td>\n",
       "      <td>3938</td>\n",
       "      <td>A9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.937752</td>\n",
       "      <td>0.347305</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>3</td>\n",
       "      <td>17794</td>\n",
       "      <td>A3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.485302</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>7</td>\n",
       "      <td>15077</td>\n",
       "      <td>P61</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3  4  5    6    7    8    9   ...   30  \\\n",
       "0      -0.5 -0.866025  0.724793  0.688967  0  1  0.0  0.0  1.0  0.0  ...  0.0   \n",
       "1       0.0      -1.0  0.201299   0.97953  0  1  0.0  1.0  0.0  0.0  ...  0.0   \n",
       "2  0.866025      -0.5  0.101168 -0.994869  0  0  1.0  0.0  0.0  0.0  ...  0.0   \n",
       "3      -0.5 -0.866025  0.937752  0.347305  0  0  1.0  0.0  0.0  0.0  ...  0.0   \n",
       "4       0.0      -1.0 -0.485302 -0.874347  1  1  0.0  0.0  0.0  1.0  ...  0.0   \n",
       "\n",
       "    31   32   33   34    35 36     37   38 39  \n",
       "0  1.0  0.0  0.0  0.0  2008  2  17592  C53  3  \n",
       "1  1.0  0.0  0.0  0.0  2008  6  12203   B9  1  \n",
       "2  0.0  0.0  1.0  0.0  2008  5   3938   A9  1  \n",
       "3  1.0  0.0  0.0  0.0  2008  3  17794   A3  1  \n",
       "4  1.0  0.0  0.0  0.0  2008  7  15077  P61  4  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the transformed array back to a DataFrame for easy viewing\n",
    "# Note: In a real pipeline, you often use the preprocessor directly in the Pipeline\n",
    "\n",
    "X_processed_df = pd.DataFrame(X_processed)\n",
    "X_processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304127fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f774731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_pipe = Pipeline(steps=[('standard_scaler', StandardScaler()),\n",
    "                ('classifier', LogisticRegression())])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clickstream_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
