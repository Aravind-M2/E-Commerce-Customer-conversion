{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session plan\n",
    "# Create a pipeline for the regression model \n",
    "#   it should pre-process and also train the model \n",
    "#   use it to predict a outcome\n",
    "# Use the created pipeline to train the model and log it with mlflow.\n",
    "# Use the logged model to predict a an outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6af3c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from scipy.stats import skew\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, chi2_contingency, f_oneway\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "import numpy as np\n",
    "from scipy.stats import boxcox, skew\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a167d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:\\\\GIT HUB\\\\GUVI Mini Proj 4\\\\train_data - train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0021f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data.drop(columns=['price'])\n",
    "df_test = data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eec6bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df_train,df_test, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "326f034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Building Pipeline for preprocessing categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d383b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be done: Grouping, Target encoding, OHE,Binary,label, cyclic encoding and Scaling on target encoded\n",
    "# done:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a5894fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cyclic good\n",
    "# cyclic encoding function for month and day\n",
    "import numpy as np\n",
    "\n",
    "def cyclic_encode(df, col, max_val):\n",
    "    \"\"\"\n",
    "    Applies sine and cosine transformation to a single column for cyclic encoding.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        col (str): The column to transform (e.g., 'hour').\n",
    "        max_val (int): The maximum possible value of the feature (e.g., 24 for hour).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the two new encoded columns.\n",
    "    \"\"\"\n",
    "    df[col + '_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
    "    df[col + '_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
    "    # Drop the original column as it is now represented by the sine and cosine features\n",
    "    df = df.drop(col, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e758d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Defining the pre-processing steps ---\n",
    "\n",
    "# Define a function to apply the encoding to the dataframe.\n",
    "# We wrap the specific logic within a lambda for the ColumnTransformer\n",
    "# to only pass the required column and context.\n",
    "def apply_cyclic_encoding(X, col, max_val):\n",
    "    # X is a numpy array when passed through ColumnTransformer, so convert it back to DataFrame\n",
    "    # Note: ColumnTransformer passes the *entire* column/array slice.\n",
    "    X_df = pd.DataFrame(X, columns=[col])\n",
    "    return cyclic_encode(X_df, col, max_val)\n",
    "\n",
    "# Create the transformers\n",
    "cyclic_hour_transformer = FunctionTransformer(\n",
    "    func=lambda X: apply_cyclic_encoding(X, 'month', 12),\n",
    "    validate=False # Set to False to handle DataFrame/numpy array mix\n",
    ")\n",
    "\n",
    "cyclic_day_transformer = FunctionTransformer(\n",
    "    func=lambda X: apply_cyclic_encoding(X, 'day', 31),\n",
    "    validate=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1145e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping, Target encoding, OHE,Binary,label and Scaling\n",
    "# done:cyclic encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d4bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old code\n",
    "# Groupping \n",
    "# import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class RareCategoryGrouper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Groups categories in a specified column that fall below a minimum\n",
    "    percentage threshold into a single 'RARE_GROUP'.\n",
    "    \"\"\"\n",
    "    def __init__(self, column_name, min_percentage=0.01, rare_label='RARE_GROUP'):\n",
    "        self.column_name = column_name\n",
    "        self.min_percentage = min_percentage\n",
    "        self.rare_label = rare_label\n",
    "        self.rare_categories_ = None # Stores the categories learned during fit\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Calculates and stores the list of categories that are below the\n",
    "        min_percentage threshold in the training data (X).\n",
    "        \"\"\"\n",
    "        # 1. Calculate the normalized frequency counts\n",
    "        value_counts = X[self.column_name].value_counts(normalize=True)\n",
    "\n",
    "        # 2. Identify and store the categories to be grouped\n",
    "        self.rare_categories_ = value_counts[value_counts < self.min_percentage].index.tolist()\n",
    "\n",
    "        if not self.rare_categories_:\n",
    "            print(f\"FIT: No categories in '{self.column_name}' below the {self.min_percentage*100:.2f}% threshold.\")\n",
    "        else:\n",
    "             print(f\"FIT: Identified {len(self.rare_categories_)} rare categories in '{self.column_name}'.\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Applies the grouping transformation using the stored rare categories.\n",
    "        \"\"\"\n",
    "        # Ensure we operate on a copy to avoid side effects on the input DataFrame\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # If no rare categories were found during fit, return the DataFrame unchanged\n",
    "        if not self.rare_categories_:\n",
    "            return X_copy\n",
    "        \n",
    "        # 3. Create the new column name\n",
    "        new_column_name = f'{self.column_name}_grouped'\n",
    "        X_copy[new_column_name] = X_copy[self.column_name]\n",
    "\n",
    "        # 4. Apply the grouping using .loc\n",
    "        X_copy.loc[X_copy[self.column_name].isin(self.rare_categories_), new_column_name] = self.rare_label\n",
    "        \n",
    "        # IMPORTANT: Drop the original column to maintain a consistent column count in the pipeline\n",
    "        X_copy = X_copy.drop(columns=[self.column_name])\n",
    "        \n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a60621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old code\n",
    "# --- Define the Grouping Steps ---\n",
    "# Group 'country' with a 1% threshold\n",
    "country_grouper = RareCategoryGrouper(column_name='country', min_percentage=0.01)\n",
    "\n",
    "# Group 'page2_clothing_model' with a 0.5% threshold\n",
    "model_grouper = RareCategoryGrouper(column_name='page2_clothing_model', min_percentage=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0a7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Target Encoder with CV and Box-Cox ---\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from category_encoders import TargetEncoder\n",
    "from scipy.stats import boxcox # For Box-Cox transformation\n",
    "class TargetEncoderCV(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Performs K-Fold Cross-Validated Target Encoding during fit, followed by Box-Cox scaling.\n",
    "    Accepts the column data from the previous pipeline step.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits=5, random_state=42):\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.full_encoder_ = None \n",
    "        self.lambda_ = None       \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # X is the data slice (now the grouped column array from RareCategoryGrouper)\n",
    "        X_series = pd.Series(X.flatten(), index=y.index)\n",
    "        \n",
    "        # 1. Cross-Validated Encoding\n",
    "        kf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "        X_encoded_train = pd.Series(np.nan, index=y.index)\n",
    "\n",
    "        for train_index, val_index in kf.split(X_series.to_frame(), y):\n",
    "            encoder = TargetEncoder()\n",
    "            # Fit only on the K-1 folds (uses Series access)\n",
    "            encoder.fit(X_series.iloc[train_index], y.iloc[train_index])\n",
    "            # Transform the validation fold\n",
    "            X_encoded_train.loc[val_index] = encoder.transform(X_series.iloc[val_index]).flatten()\n",
    "            \n",
    "        # 2. Box-Cox Transformation Parameter Learning\n",
    "        X_encoded_train_2d = X_encoded_train.values.reshape(-1, 1)\n",
    "        X_boxcox, self.lambda_ = boxcox(X_encoded_train_2d + 1e-6) \n",
    "\n",
    "        # 3. Store the full encoder for transformation of *new* data\n",
    "        self.full_encoder_ = TargetEncoder()\n",
    "        self.full_encoder_.fit(X_series, y)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # X is the data slice from the previous step\n",
    "        X_series = pd.Series(X.flatten(), index=pd.Index(range(len(X))))\n",
    "        \n",
    "        # 1. Apply Target Encoding using the FULL ENCODER learned during fit\n",
    "        X_encoded = self.full_encoder_.transform(X_series)\n",
    "        \n",
    "        # 2. Apply Box-Cox Transformation using the stored lambda\n",
    "        X_transformed = boxcox(X_encoded.values.reshape(-1, 1) + 1e-6, lmbda=self.lambda_)\n",
    "\n",
    "        return X_transformed.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping new\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# --- 1. Rare Category Grouper (Returns only the new column array) ---\n",
    "class RareCategoryGrouper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Groups categories that fall below a minimum percentage threshold into a single 'RARE_GROUP'.\n",
    "    This version is designed for use in a scikit-learn Pipeline and returns only the transformed column data.\n",
    "    \"\"\"\n",
    "    def __init__(self, rare_label='RARE_GROUP', min_percentage=0.01):\n",
    "        # We don't need 'column_name' here because the ColumnTransformer will select the column for us.\n",
    "        self.rare_label = rare_label\n",
    "        self.min_percentage = min_percentage\n",
    "        self.rare_categories_ = None # Stores the categories to be grouped\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # X is the slice of data (e.g., the 'page2_clothing_model_grouped' column data)\n",
    "        # Convert array slice back to Series for value_counts\n",
    "        X_series = pd.Series(X.flatten())\n",
    "\n",
    "        value_counts = X_series.value_counts(normalize=True)\n",
    "        self.rare_categories_ = value_counts[value_counts < self.min_percentage].index.tolist()\n",
    "        \n",
    "        # We don't print inside fit/transform methods in production code, but keeping for explanation:\n",
    "        # print(f\"FIT: Identified {len(self.rare_categories_)} rare categories.\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_series = pd.Series(X.flatten(), index=pd.Index(range(len(X))))\n",
    "        \n",
    "        # Create a copy to modify\n",
    "        X_transformed = X_series.copy()\n",
    "        \n",
    "        if self.rare_categories_:\n",
    "            # Apply the grouping based on the categories learned during fit\n",
    "            X_transformed[X_series.isin(self.rare_categories_)] = self.rare_label\n",
    "        \n",
    "        # Return the transformed column data as a 2D array, which is required by scikit-learn\n",
    "        return X_transformed.values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1430127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define the Mini-Pipeline for Sequential Steps ---\n",
    "# This pipeline performs two actions on the same column sequentially: Grouping then Encoding/Scaling\n",
    "target_encoding_and_scaling_pipe = Pipeline(steps=[\n",
    "    # Step 1: Group rare categories in the selected column\n",
    "    ('group_rare', RareCategoryGrouper(min_percentage=0.005)), \n",
    "    # Step 2: Apply K-Fold Target Encoding and Box-Cox Scaling to the grouped result\n",
    "    ('target_boxcox', TargetEncoderCV(n_splits=5))\n",
    "])\n",
    "\n",
    "country_grouper_OHE_pipe = Pipeline(steps = [('group_rare', RareCategoryGrouper(min_percentage= 0.01)),\n",
    "                                             ('ohe_country', ohe_encoder())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e659a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the grouping+target encoder+boxcox pipeline \n",
    "# complete the grouping + OHE\n",
    "# Complete the OHE\n",
    "# compile the 1st 2 pipelines with other steps in the columntransformer\n",
    "# compile the final pre-processer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f8f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoding, OHE,label and Scaling\n",
    "# done:cyclic encoding , Grouping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2e46fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding  - good\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class DictionaryMapper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Applies a fixed dictionary mapping (Label Encoding) to a specified column.\n",
    "    \"\"\"\n",
    "    def __init__(self, column_name, mapping_dict):\n",
    "        self.column_name = column_name\n",
    "        self.mapping_dict = mapping_dict\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No fitting is required as the mapping is pre-defined\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Apply the mapping and replace the original column\n",
    "        X_copy[self.column_name] = X_copy[self.column_name].map(self.mapping_dict)\n",
    "        \n",
    "        # NOTE: .map() will produce NaN for values not in the dictionary. \n",
    "        # If your data might contain other values, you may need a fillna() or error check here.\n",
    "        \n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3cc4a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mappers using your specified dictionary\n",
    "model_photo_mapper = DictionaryMapper(\n",
    "    column_name='model_photography', \n",
    "    mapping_dict={1: 0, 2: 1}\n",
    ")\n",
    "\n",
    "price_mapper = DictionaryMapper(\n",
    "    column_name='price_2', \n",
    "    mapping_dict={1: 0, 2: 1}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoding, OHE and Scaling\n",
    "# done:cyclic encoding , Grouping, label encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f54bc48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#good - grouping\n",
    "# --- Custom Transformer for Rare Category Grouping (FIXED) ---\n",
    "class RareCategoryGrouper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer to group categories whose frequency is below a given threshold\n",
    "    into a single 'RARE_GROUP' category.\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold=0.01, rare_group_name='RARE_GROUP'):\n",
    "        # The threshold (e.g., 0.01 for 1%) below which categories are grouped\n",
    "        self.threshold = threshold\n",
    "        # The name assigned to the grouped rare categories\n",
    "        self.rare_group_name = rare_group_name\n",
    "        # Internal attribute to store the list of frequent categories (learned during fit)\n",
    "        self.frequent_categories_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Determine the Series to work with (handles DataFrame or Series input)\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # If it's a DataFrame, extract the first (and only) column\n",
    "            X_series = X.iloc[:, 0]\n",
    "        else:\n",
    "            X_series = X\n",
    "        \n",
    "        # Calculate the frequency of each category\n",
    "        category_counts = X_series.value_counts(normalize=True)\n",
    "        \n",
    "        # Identify categories that meet or exceed the threshold\n",
    "        self.frequent_categories_ = category_counts[category_counts >= self.threshold].index.tolist()\n",
    "        \n",
    "        # Return self to adhere to scikit-learn API\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # --- FIX APPLIED HERE ---\n",
    "        # 1. Determine the column content to transform (Series)\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # Extract the single column content as a Series\n",
    "            series_to_process = X.iloc[:, 0]\n",
    "            name_to_keep = X.columns[0]\n",
    "        else: # Must be a Series\n",
    "            series_to_process = X\n",
    "            name_to_keep = X.name\n",
    "            \n",
    "        # Perform the actual grouping transformation on the Series data (using numpy.where)\n",
    "        grouped_values = np.where(\n",
    "            series_to_process.isin(self.frequent_categories_), \n",
    "            series_to_process, \n",
    "            self.rare_group_name\n",
    "        )\n",
    "\n",
    "        # 2. Re-wrap the results as a Series, maintaining index and original column name\n",
    "        X_transformed_series = pd.Series(\n",
    "            grouped_values, \n",
    "            index=X.index, \n",
    "            name=name_to_keep\n",
    "        )\n",
    "        \n",
    "        # 3. Always return a DataFrame (2D array-like structure) by calling to_frame() on the Series\n",
    "        return X_transformed_series.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae8de2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Grouping_OHE_pipeline = Pipeline(steps=[\n",
    "    (\"Grouping\",RareCategoryGrouper(threshold=0.01)),\n",
    "    (\"OHE\",OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cdbe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoding - only one column left(clothing model)\n",
    "# done:cyclic encoding , Grouping, label encoding ,OHE (applied directly on the pipeline and column tranformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a14eba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoding \n",
    "# --- 1. KFold Target Encoder Class (Copied from target_encoding.py) ---\n",
    "# NOTE: This class is essential for the pipeline to run correctly.\n",
    "from sklearn.model_selection import KFold\n",
    "class KFoldTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Performs K-Fold Target Encoding for a single categorical column \n",
    "    while preventing data leakage using cross-validation.\n",
    "    \n",
    "    This encoder is suitable for regression tasks where the target (y) is continuous.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=42, smoothing=None):\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        self.smoothing = smoothing\n",
    "        self._kf = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "        self._mapping = {}\n",
    "        self._global_mean = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "             X = X.iloc[:, 0]\n",
    "        y_series = pd.Series(y)\n",
    "        \n",
    "        self._global_mean = y_series.mean()\n",
    "        self._mapping = y_series.groupby(X).mean().to_dict()\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "             X = X.iloc[:, 0]\n",
    "             \n",
    "        encoded_feature = X.map(self._mapping).fillna(self._global_mean)\n",
    "        \n",
    "        # Ensure output is a DataFrame/Series so it works well in ColumnTransformer\n",
    "        return encoded_feature.to_frame(name=X.name + '_encoded')\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "             X_series = X.iloc[:, 0].copy()\n",
    "        else:\n",
    "             X_series = X.copy()\n",
    "             \n",
    "        y = pd.Series(y)\n",
    "        X_encoded = pd.Series(np.zeros(len(X_series)), index=X_series.index)\n",
    "        \n",
    "        # 1. Iterate over K-Folds\n",
    "        for train_idx, val_idx in self._kf.split(X_series, y):\n",
    "            X_train_fold, X_val_fold = X_series.iloc[train_idx], X_series.iloc[val_idx]\n",
    "            y_train_fold = y.iloc[train_idx]\n",
    "            \n",
    "            # 2. Calculate the mean target based ONLY on the current train fold\n",
    "            fold_means = y_train_fold.groupby(X_train_fold).mean()\n",
    "            global_mean_fold = y_train_fold.mean()\n",
    "            \n",
    "            # 3. Map these means to the validation fold\n",
    "            X_encoded.iloc[val_idx] = X_val_fold.map(fold_means).fillna(global_mean_fold)\n",
    "            \n",
    "        # 4. Fit the final, overall mapping for future 'transform' calls (on test data)\n",
    "        self.fit(X_series, y) \n",
    "\n",
    "        # Return as DataFrame for ColumnTransformer compatibility\n",
    "        return X_encoded.to_frame(name=X_series.name + '_encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca5119e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Grouping_TargetEncode_pipeline = Pipeline(steps=[\n",
    "    (\"Grouping\",RareCategoryGrouper(threshold=0.005)),\n",
    "    (\"Target encoding\",KFoldTargetEncoder(n_splits=5, random_state=42))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23ff9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Build the ColumnTransformer ---\n",
    "# Categorical \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Apply cyclic encoding to 'hour' and 'day_of_week'\n",
    "        ('cyclic_hour', cyclic_hour_transformer, ['month']),\n",
    "        ('cyclic_day', cyclic_day_transformer, ['day']),\n",
    "\n",
    "        # Label Encoding (NO SCALING APPLIED)\n",
    "        # Use the mappers directly here. The output is 0/1, which is final.\n",
    "        ('map_model_photo', model_photo_mapper, ['model_photography']),\n",
    "        ('map_price_2', price_mapper, ['price_2']),\n",
    "\n",
    "        # ONE-HOT ENCODING STEP (New) \n",
    "        ('simple OHE', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['page1_main_category','colour','location']),\n",
    "        \n",
    "        # Grouping pipe\n",
    "        (\"Grouping+OHE\",Grouping_OHE_pipeline,['country']),\n",
    "\n",
    "        # Target Encoding for the categorical feature (leakage-free)\n",
    "        ('target_encoder', Grouping_TargetEncode_pipeline, ['page2_clothing_model'])\n",
    "\n",
    "    ],\n",
    "    # remainder='passthrough' # Keep any other columns if they exist\n",
    "    remainder='drop'# Drop all other columns not explicitly listed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f613606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "core_pipeline = Pipeline(steps=[\n",
    "    # ... your existing Grouping, OHE, Target Encoding, and numeric steps ...\n",
    "    # Placeholder estimator:\n",
    "    ('Preprocessor', preprocessor),\n",
    "    ('estimator', LinearRegression()) # Changed from Ridge(random_state=42)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91d19285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the Box-Cox Transformer\n",
    "# We use PowerTransformer from sklearn and set the method to 'box-cox'.\n",
    "# The Box-Cox method requires the input data (the target Y) to be strictly positive (Y > 0).\n",
    "# If your target can be zero or negative, you might need to shift it (e.g., Y_shifted = Y + 1)\n",
    "# or consider the 'yeo-johnson' method instead.\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "boxcox_transformer = PowerTransformer(method='box-cox', standardize=False)\n",
    "\n",
    "# 2. Create the TransformedTargetRegressor (TTR) wrapper\n",
    "# The TTR handles:\n",
    "# 1. fit: Y -> fit(boxcox_transformer) -> Y_transformed\n",
    "# 2. predict: X -> core_pipeline -> Y_transformed_hat\n",
    "# 3. inverse_transform: Y_transformed_hat -> inverse_transform(boxcox_transformer) -> Y_hat (final prediction)\n",
    "final_model = TransformedTargetRegressor(\n",
    "    regressor=core_pipeline,\n",
    "    transformer=boxcox_transformer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25205571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the TransformedTargetRegressor...\n",
      "Fit complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Training and Prediction ---\n",
    "\n",
    "# 1. Fit the final model (TTR intercepts Y_train and transforms it to log(Y_train))\n",
    "print(\"Fitting the TransformedTargetRegressor...\")\n",
    "final_model.fit(X_train, Y_train)\n",
    "print(\"Fit complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbcccbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GIT HUB\\GUVI Mini Proj 4\\clickstream_venv\\Lib\\site-packages\\sklearn\\pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 2. Predict (TTR automatically applies the inverse exp() function to the predictions)\n",
    "Y_pred = final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f03af257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.42430767963361\n"
     ]
    }
   ],
   "source": [
    "# 3. Evaluate results on the original scale\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "rmse = mean_squared_error(Y_test, Y_pred)\n",
    "print(rmse)\n",
    "\n",
    "# print(\"\\n--- Model Performance ---\")\n",
    "# print(f\"Test Root Mean Squared Error (RMSE) on original scale: ${rmse:.2f}\")\n",
    "\n",
    "# print(\"\\n--- Prediction Comparison (Original Scale) ---\")\n",
    "# results = pd.DataFrame({\n",
    "#     'Actual Price': Y_test,\n",
    "#     'Predicted Price': Y_pred\n",
    "# })\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f472c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.5819, MSE: 23.4243, RMSE: 4.8399, R2 Score: 0.8522\n"
     ]
    }
   ],
   "source": [
    "# 3. Predict and Evaluate\n",
    "# y_pred = best_estimator.predict(x_test)\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "print(f\"MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf0d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test run to validate the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65e91fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"D:\\\\GIT HUB\\\\GUVI Mini Proj 4\\\\test_data - test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83599e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test_data.drop(columns= ['price'])\n",
    "y = test_data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3758208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26476,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6ae2d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GIT HUB\\GUVI Mini Proj 4\\clickstream_venv\\Lib\\site-packages\\sklearn\\pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_pred = final_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a714b37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Performance ---\n",
      "Test Root Mean Squared Error (RMSE) on original scale: $4.84\n",
      "\n",
      "--- Prediction Comparison (Original Scale) ---\n",
      "       Actual Price  Predicted Price\n",
      "0                33        27.086633\n",
      "1                33        36.158185\n",
      "2                28        26.407764\n",
      "3                57        58.962233\n",
      "4                43        40.968232\n",
      "...             ...              ...\n",
      "33090            43        39.128288\n",
      "33091            33        36.172293\n",
      "33092            48        38.535691\n",
      "33093            48        48.286674\n",
      "33094            33        28.920606\n",
      "\n",
      "[33095 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Model Performance ---\")\n",
    "print(f\"Test Root Mean Squared Error (RMSE) on original scale: ${rmse:.2f}\")\n",
    "\n",
    "print(\"\\n--- Prediction Comparison (Original Scale) ---\")\n",
    "results = pd.DataFrame({\n",
    "    'Actual Price': y,\n",
    "    'Predicted Price': y_pred\n",
    "})\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3674f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.6023, MSE: 23.6831, RMSE: 4.8665, R2 Score: 0.8504\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(y, y_pred)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y, y_pred)\n",
    "print(f\"MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07615fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below code is for verification used while building pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaff20d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed1 = preprocessor.fit_transform(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac56ddbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4999999999999997, -0.8660254037844388, 0.72479278722912,\n",
       "        0.6889669190756866, 0, 1, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "        0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n",
       "        0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 2008, 2, 17592, 'C53', 3],\n",
       "       [1.2246467991473532e-16, -1.0, 0.20129852008866006,\n",
       "        0.9795299412524945, 0, 1, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n",
       "        0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,\n",
       "        0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 2008, 6, 12203, 'B9', 1]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_processed1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5615057d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.00000000e-01, -8.66025404e-01,  7.24792787e-01,\n",
       "         6.88966919e-01,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  3.97739924e+01,\n",
       "         2.00800000e+03,  2.00000000e+00,  1.75920000e+04,\n",
       "         3.00000000e+00],\n",
       "       [ 1.22464680e-16, -1.00000000e+00,  2.01298520e-01,\n",
       "         9.79529941e-01,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  4.80000000e+01,\n",
       "         2.00800000e+03,  6.00000000e+00,  1.22030000e+04,\n",
       "         1.00000000e+00]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_processed[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce41c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clickstream_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
